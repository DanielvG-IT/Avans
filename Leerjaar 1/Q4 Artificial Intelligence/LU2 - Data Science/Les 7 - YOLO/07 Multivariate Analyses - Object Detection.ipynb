{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6706be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Ignore the ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933067f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6e5112-3a04-4168-8bfe-fea763163f09",
   "metadata": {},
   "source": [
    "# Object Detection\n",
    "#### Introduction to Object Detection\n",
    "\n",
    "Object detection is a computer vision task that involves identifying and locating objects within an image or video. Unlike image classification, which only assigns a label to an image, object detection provides both the class label and the bounding box coordinates of objects present in the image.\n",
    "\n",
    "### YOLO (You Only Look Once)\n",
    "\n",
    "YOLO (You Only Look Once) is a popular real-time object detection algorithm known for its speed and accuracy. Unlike traditional object detection methods that use region proposals, YOLO treats object detection as a single regression problem, predicting bounding boxes and class probabilities directly from the image in one pass.\n",
    "\n",
    "#### How YOLO Works\n",
    "\n",
    "1. **Image Grid Division**: The input image is divided into an \\( S \\times S \\) grid.\n",
    "2. **Bounding Box Prediction**: Each grid cell predicts a fixed number of bounding boxes along with confidence scores.\n",
    "3. **Class Probability Prediction**: Each bounding box is assigned a class label with a probability.\n",
    "4. **Non-Maximum Suppression (NMS)**: Overlapping boxes are filtered to retain the most confident detections.\n",
    "\n",
    "#### Advantages of YOLO\n",
    "\n",
    "- **Fast Inference**: YOLO can process images in real-time, making it suitable for applications like autonomous driving and video surveillance.\n",
    "- **End-to-End Training**: Unlike region-based methods, YOLO is trained as a single neural network, making it more efficient.\n",
    "- **Global Context Awareness**: Since YOLO processes the entire image at once, it reduces false positives compared to sliding window or region-based methods.\n",
    "\n",
    "#### Variants of YOLO\n",
    "\n",
    "- **YOLOv1-v3**: Initial versions that introduced the concept and improved accuracy.\n",
    "- **YOLOv4**: Optimized for better speed and precision.\n",
    "- **YOLOv5**: A widely used implementation with improvements in efficiency.\n",
    "- **YOLOv7 & YOLOv8**: State-of-the-art versions focusing on faster inference and higher detection accuracy.\n",
    "\n",
    "Due to its real-time performance and high accuracy, YOLO is widely used in applications such as autonomous vehicles, security systems, and medical diagnostics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51afce4-2afe-498e-a945-4f4b7466584e",
   "metadata": {},
   "source": [
    "## Applying YOLO algorithm\n",
    "We are going to apply a YOLO device that would be applicable for a self driving car, if it knows what objects are around him on the high way it can help out driving safely.\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*XNhy_ZX7h8D8eJrDuBkTWQ.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea648e6-333b-48be-a709-157bef251f2d",
   "metadata": {},
   "source": [
    "### Take a look in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8c7898-b032-4f99-bdcb-cfd10deeeca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9a9525-cac9-4091-bf70-4e7655336fa4",
   "metadata": {},
   "source": [
    "In each algorithm we have an input (features) and a output (target), we can define them also als X, y respectively. Now lets have a quick dive into the dataset before we apply YOLO.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba99494a-cc11-47cc-b519-33b2f44364c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Construct the base path dynamically\n",
    "base_path = os.path.join(current_dir, '../Datasets/TrafficSignRecognition/car/train/images')\n",
    "\n",
    "# Get the first image file in the folder\n",
    "image_files = os.listdir(base_path)  # List all files in the folder\n",
    "\n",
    "print(\"Image files in the folder:\", len(image_files))  # Print the list of image files\n",
    "first_image_path = os.path.join(base_path, image_files[0])  # Get the path of the first image\n",
    "\n",
    "# Read the first image\n",
    "image = cv2.imread(first_image_path)\n",
    "\n",
    "# Convert from BGR (OpenCV) to RGB (matplotlib)\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the first image\n",
    "plt.imshow(image_rgb)\n",
    "plt.axis('off')  # Hide axes for a cleaner look\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b95230-838e-45fa-a24f-5ab8b7598742",
   "metadata": {},
   "source": [
    "Remember the lecture? We had different vectors that we used as targets. Now, we will gather data from the images, using the data from the center of the grid, which corresponds to a specific label that we want our model to predict. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b578aba4-f798-431c-a491-3c7e9b750ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "#Load the labels which are vectors with a reference to the image\n",
    "df_target = pd.read_csv('labels_train.csv')\n",
    "df_target = shuffle(df_target, random_state=34)\n",
    "df_target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad75f506",
   "metadata": {},
   "source": [
    "**What are these columns?**\n",
    "\n",
    "The filename is stored (frame), the class it belongs to (class_id) and some bounding box coordinates.\n",
    "A bounding box is a rectangular region used to identify and isolate an object within an image. **It is defined by four coordinates: xmin, xmax, ymin, and ymax.** These coordinates specify the horizontal and vertical extents of the box, effectively creating a frame around the object of interest. In the code, these coordinates are stored for each image to precisely indicate where the object is located within the image. By keeping these bounding box coordinates, the program can later use them to draw or highlight the detected object, making it essential for tasks like object detection and image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc939e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_target.shape) # print the shape of the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4939fe04-5121-44f4-a88a-9f23a21e7466",
   "metadata": {},
   "source": [
    "This dataset is mostly a practice and training aspect for the subject of YOLO. Let's figure out how many different classes there are in the dataset and how often they appear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c4a185-8579-4ac0-bdcc-3d2f44728d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df_target.class_id.unique()\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a582fd45-fa51-4e72-bd44-09e8eb83776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target['class_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e67738-5f01-4a91-9e10-b9bcd21819f4",
   "metadata": {},
   "source": [
    "The class_id labels seems to be: 'car', 'truck', 'pedestrian', 'bicyclist', 'light'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b40465-63f2-414a-9185-294b10b5b652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transfer the numbers into the 5 different classes \n",
    "labels = { 1:'car', 2:'truck', 3:'person', 4:'bicycle', 5:'traffic light'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3384edd0-74a5-4f17-a049-82836891b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = {}\n",
    "images = {}\n",
    "\n",
    "print(df_target.head())\n",
    "\n",
    "for class_id in classes:\n",
    "    # get all the rows for the current class\n",
    "    print(f\"Processing class {class_id}...\")\n",
    "    target_rows = df_target[df_target['class_id'] == class_id]\n",
    "    print(f\"Number of rows for class {class_id}: {len(target_rows)}\")\n",
    "    # Check if there are any rows for the current class. If not, skip to the next class   \n",
    "    if target_rows.empty:\n",
    "        print(f\"No data found for class {class_id}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Search for first image on disk from the dataframe\n",
    "    image_found = False\n",
    "    for _, row in target_rows.iterrows():\n",
    "        file_name = row['frame']\n",
    "        image_path = os.path.join(base_path, file_name)\n",
    "        \n",
    "        if not os.path.exists(image_path):\n",
    "            continue\n",
    "        \n",
    "        print(f\"Found image {image_path} for class {class_id}.\")\n",
    "\n",
    "        # Lees de afbeelding\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        if image is None:\n",
    "            print(f\"Failed to load image {image_path}. Trying next row...\")\n",
    "            continue\n",
    "        else:\n",
    "            image_found = True\n",
    "            break\n",
    "        \n",
    "    if image_found:\n",
    "        print(f\"Image found AND read for class {class_id}: {image_path}\")\n",
    "        # Convert from BGR (OpenCV) to RGB (matplotlib)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        images[class_id] = image_rgb\n",
    "        boxes[class_id] = [\n",
    "            int(row['xmin']), \n",
    "            int(row['xmax']), \n",
    "            int(row['ymin']), \n",
    "            int(row['ymax'])\n",
    "        ]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f905bc",
   "metadata": {},
   "source": [
    "**What is happening here with the colors of the image?**\n",
    "\n",
    "When an image is loaded using OpenCV (cv2.imread), it is automatically stored in BGR (Blue, Green, Red) format, which is OpenCV's default color ordering. However, most image processing and visualization libraries, such as matplotlib, use RGB (Red, Green, Blue) format. To ensure the image colors are displayed correctly, the code converts the image from BGR to RGB using cv2.cvtColor(image, cv2.COLOR_BGR2RGB). This simply reorders the color channels of the image, making it compatible with other libraries that expect RGB format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256211ac-6b51-4d0b-839c-ee746d9b7f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's visualise the 5 different classes that exists with their bounding boxes in a image \n",
    "for class_id in classes:\n",
    "    xmin, xmax, ymin, ymax = boxes[class_id][0], boxes[class_id][1], boxes[class_id][2], boxes[class_id][3]\n",
    "\n",
    "    plt.figure(figsize=(4, 6))\n",
    "    plt.title(\"Label \" + labels[class_id])\n",
    "    plt.imshow(images[class_id])\n",
    "    plt.gca().add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color='yellow', fill=False, linewidth=2))\n",
    "\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e276736e-146a-4f2d-9f58-0a14a6b415c5",
   "metadata": {},
   "source": [
    "### Applying YOLO on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe0d3e1-0881-42bd-9829-871c4cf0796f",
   "metadata": {},
   "source": [
    "Lets load in a YOLO v8 model therefor we need first to install the package ultralitics do this by using '!pip install ultralytics'. Afterwards we load in the necessary packages to visualise the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d373c439-2371-413d-96d8-88a4c79c7f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48770971-cfd3-4ff0-9654-7977ffe6c111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import PIL \n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import os \n",
    "import pathlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640f376c-c3d5-43ff-a601-a7db36550b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YOLO v8 model\n",
    "model = YOLO(\"yolov8m.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ef8177-fb6c-47ee-b33a-971a3f0cf939",
   "metadata": {},
   "source": [
    "Let's select a random image from the 100 images in the \"images\" folder to use as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6148f022-b64e-46c3-ae79-d60a1f6a6675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(80) \n",
    "random_num=np.random.randint(0, 100)\n",
    "example = os.path.join(base_path, image_files[random_num])  # Get the path of the first image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5281a9-61aa-447d-8039-1478682bb40f",
   "metadata": {},
   "source": [
    "Now since we have an image and we can let YOLO do a prediction on the dataset. You can improve YOLO's performance, by adjusting the confidence (conf) and IoU thresholds.\n",
    "\n",
    "Increasing these thresholds will:\n",
    "- Higher Confidence: Increases accuracy by being more selective, but may overlook some valid detections.\n",
    "- Higher IoU: Ensures stricter overlap criteria, reducing incorrect identifications, but may exclude some correct ones if the overlap isn't sufficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e6e756-12a6-4778-8441-5df305a03b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(source=example, save=True, conf=0.2, iou=0.2) # change conf and iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7fb36d-4690-4efb-bee2-a69932d03f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the information (class, coordinates of the box and the confidence) of each predicted object\n",
    "result = results[0]\n",
    "for box in result.boxes:\n",
    "    class_id = result.names[box.cls[0].item()]\n",
    "    cords = box.xyxy[0].tolist()\n",
    "    cords = [round(x) for x in cords]\n",
    "    conf = round(box.conf[0].item(), 2)\n",
    "    print(\"Object type:\", class_id)\n",
    "    print(\"Coordinates:\", cords)\n",
    "    print(\"Probability:\", conf)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c090a026-ccff-4b9a-a1ef-18d63c4eb305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the predicted results\n",
    "plot = results[0].plot()\n",
    "\n",
    "# Convert the plot from BGR to RGB\n",
    "plot = cv2.cvtColor(plot, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Create a larger figure size\n",
    "plt.figure(figsize=(10, 10))  \n",
    "plt.imshow(plot)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbc0249-a4a1-45dd-8805-0e0465ebdb24",
   "metadata": {},
   "source": [
    "Now, let's visualize the actual objects in the image, identify their classes, and show their locations and orientations. We'll plot these details on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075f0ad9-d553-4f76-a5f3-35b009ccba27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the filename\n",
    "filename = os.path.basename(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2ae3d3-47f8-4e9d-94bf-16bdb67ff119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the target data corresponding to the image\n",
    "df_target[df_target['frame']==filename]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e895042e-5298-4969-b6fa-a6ae06eed4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all matching rows in df_target for this filename\n",
    "matching_rows = df_target[df_target['frame'] == filename]\n",
    "\n",
    "# Construct the full image path\n",
    "image_path = os.path.join(base_path, filename)\n",
    "\n",
    "# Read and convert the image\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB for Matplotlib\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "\n",
    "for index, row in matching_rows.iterrows():\n",
    "    xmin, xmax, ymin, ymax = row['xmin'], row['xmax'], row['ymin'], row['ymax']\n",
    "    \n",
    "    plt.gca().add_patch(\n",
    "        plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, \n",
    "                      edgecolor='yellow', fill=False, linewidth=2)\n",
    "    )\n",
    "\n",
    "    # Add the index number above the bounding box\n",
    "    plt.text(xmin, ymin - 5, f\"{index}\", color='yellow', \n",
    "             fontsize=12, backgroundcolor='none')\n",
    "\n",
    "plt.axis('off')  # Hide axes for a cl\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7f6505-b260-48c3-bff1-fb0a63b2e9cf",
   "metadata": {},
   "source": [
    "We need to transform YOLO's predicted bounding boxes into the format required for our dataset. YOLO uses its own object IDs, which are mapped to custom class IDs (1 to 5) for consistency with our dataset. The code extracts YOLO's center coordinates (xywh) and converts them into xmin, ymin, xmax, ymax format. Then, it maps YOLO's class IDs to our custom class IDs using a predefined dictionary. Finally, the processed predictions are stored in a pandas DataFrame for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e056416-3158-4d69-988f-4d406ae214d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the mapping from YOLO class IDs to the new custom label IDs\n",
    "yolo_to_custom = {\n",
    "    2: 1,  # Car -> Class ID 2 maps to 1\n",
    "    7: 2,  # Truck -> Class ID 7 maps to 2\n",
    "    0: 3,  # Person -> Class ID 0 maps to 3\n",
    "    1: 4,  # Bicycle -> Class ID 1 maps to 4\n",
    "    9: 5   # Traffic light -> Class ID 9 maps to 5\n",
    "}\n",
    "\n",
    "# Initialize a list to hold the rows of the DataFrame\n",
    "predictions_data = []\n",
    "\n",
    "# Example: Extract YOLO predictions and ground truth for each frame (this should come from results[0])\n",
    "for i, box in enumerate(results[0].boxes):  # Iterate through each prediction\n",
    "    frame = filename  # You would retrieve the frame name here\n",
    "    \n",
    "    # Extract xywh (center_x, center_y, width, height)\n",
    "    center_x, center_y, width, height = box.xywh[0].int().tolist()  # Convert to integers\n",
    "    \n",
    "    # Convert to xmin, ymin, xmax, ymax\n",
    "    xmin = int(center_x - width / 2)\n",
    "    ymin = int(center_y - height / 2)\n",
    "    xmax = int(center_x + width / 2)\n",
    "    ymax = int(center_y + height / 2)\n",
    "    \n",
    "    # Convert class_id tensor to integer\n",
    "    class_id = box.cls.int().item()  # Convert class tensor to integer\n",
    "    \n",
    "    # Map YOLO class_id to the custom label ID\n",
    "    custom_class_id = yolo_to_custom.get(class_id, -1)  # Get the custom label ID (-1 for unknown)\n",
    "    \n",
    "    # Append the prediction data to the list, now without the class_label\n",
    "    predictions_data.append([frame, xmin, xmax, ymin, ymax, custom_class_id])\n",
    "\n",
    "# Convert the predictions data to a pandas DataFrame\n",
    "predictions_df = pd.DataFrame(predictions_data, columns=['frame', 'xmin', 'xmax', 'ymin', 'ymax', 'class_id'])\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2538e2-4d05-47f7-ad62-9cbee5d0b653",
   "metadata": {},
   "source": [
    "### Lets evaluate the default YOLO algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bede764d-674e-4d8d-a2b8-d5826fa51bbb",
   "metadata": {},
   "source": [
    "Let's now examine the performance of YOLO. While there are various methods to evaluate its performance, most of them rely on the **confusion matrix**. We'll build a confusion matrix to assess our predicted data. To understand the importance of this, consider the scenario with Tim, who was fined for holding a phone when he actually wasn’t this by the YOLO algorithm which was used by the dutch police. [Handsfree detection algorithm at the dutch police](https://nippur.nl/tim-versus-politie-algoritme/) \n",
    "\n",
    "![](https://nippur.nl/wp-content/uploads/2024/02/confusion-matrix.png)\n",
    "\n",
    "In addition to that, we also covered the concept of **Intersection of Union (IoU)**. Let's go ahead and calculate the IoU for this example as well.\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*za4Hpz8SDUdWg-S6goL4yg.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ee7bed-08e9-4d8a-bdde-40a100fe47e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate IoU\n",
    "def compute_iou(pred_box, gt_box):\n",
    "    x1, y1, x2, y2 = max(pred_box[0], gt_box[0]), max(pred_box[1], gt_box[1]), min(pred_box[2], gt_box[2]), min(pred_box[3], gt_box[3])\n",
    "    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    pred_area = (pred_box[2] - pred_box[0]) * (pred_box[3] - pred_box[1])\n",
    "    gt_area = (gt_box[2] - gt_box[0]) * (gt_box[3] - gt_box[1])\n",
    "    union_area = pred_area + gt_area - inter_area\n",
    "    return inter_area / union_area if union_area > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014e90b5-e84e-42a5-9aec-3cc00fa496f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "ground_truth_df = df_target[df_target['frame'] == filename]\n",
    "\n",
    "# List to hold indices, ground truth, and IoU values\n",
    "iou_info = []\n",
    "\n",
    "# Calculate IoU without applying a threshold\n",
    "for i, pred in predictions_df.iterrows():\n",
    "    frame = pred['frame']\n",
    "    pred_box = [pred['xmin'], pred['ymin'], pred['xmax'], pred['ymax']]\n",
    "    pred_class = pred['class_id']\n",
    "    \n",
    "    # Find the corresponding ground truth\n",
    "    gt_boxes = ground_truth_df[ground_truth_df['frame'] == frame]\n",
    "    \n",
    "    for j, gt in gt_boxes.iterrows():\n",
    "        gt_box = [gt['xmin'], gt['ymin'], gt['xmax'], gt['ymax']]\n",
    "        gt_class = gt['class_id']\n",
    "        \n",
    "        # Calculate IoU between prediction and ground truth\n",
    "        iou = compute_iou(pred_box, gt_box)\n",
    "        \n",
    "        # Store the index, ground truth, and IoU for all predictions\n",
    "        iou_info.append([i, j, pred_class, gt_class, iou])\n",
    "\n",
    "# Create DataFrame to show the IoU results with prediction and ground truth indices\n",
    "iou_df = pd.DataFrame(iou_info, columns=['prediction_index', 'ground_truth_index', 'pred_class_id', 'gt_class_id', 'iou'])\n",
    "\n",
    "# Now, we can calculate the confusion matrix for all predictions (no threshold)\n",
    "predicted_classes = iou_df['pred_class_id'].values\n",
    "actual_classes = iou_df['gt_class_id'].values\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(actual_classes, predicted_classes)\n",
    "\n",
    "# Visualize the confusion matrix using Seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Car', 'Truck', 'Person', 'Bicycle', 'Traffic Light'], yticklabels=['Car', 'Truck', 'Person', 'Bicycle', 'Traffic Light'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix (All Predictions)')\n",
    "plt.show()\n",
    "\n",
    "# Output the IoU values for all predictions\n",
    "print(\"\\nIoU per object:\")\n",
    "print(iou_df[['prediction_index', 'ground_truth_index', 'iou']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6727d373-4fd0-4cea-846a-ff27fc1b2898",
   "metadata": {},
   "source": [
    "# Fine-Tuning YOLO and Labeling Your Own Data  \n",
    "\n",
    "YOLO is already a strong object detection model, but in some cases, you may want to fine-tune it to improve performance for your specific use case. One way to do this is by using pre-trained YOLO models provided by platforms like **[Roboflow](https://roboflow.com/)**, which offer easy-to-use training pipelines. Alternatively, if your dataset is not labeled yet, you’ll need to label your own data before training. **[Explanation video about labeling](https://www.youtube.com/watch?v=a3SBRtILjPI)**\n",
    "\n",
    "##### Labeling Your Own Data  \n",
    "\n",
    "Labeling is the process of defining the target objects in your dataset by drawing bounding boxes around them and specifying their class. This is essential for training an object detection model. One of the most popular tools for labeling data is Roboflow, which allows you to upload images, annotate them, and export them in formats compatible with YOLO. To get started, sign up for a Roboflow account.  \n",
    "\n",
    "##### Other Labeling Tools  \n",
    "\n",
    "- LabelImg – A simple, open-source tool for annotating images.  \n",
    "  - [GitHub - LabelImg](https://github.com/HumanSignal/labelImg)  \n",
    "\n",
    "- CVAT (Computer Vision Annotation Tool) – A powerful tool with advanced features for annotating images and videos.  \n",
    "  - [CVAT.org](https://cvat.org/) | [GitHub - CVAT](https://github.com/opencv/cvat)  \n",
    "\n",
    "- LabelMe – Another open-source annotation tool focused on polygon-based annotations.  \n",
    "  - [GitHub - LabelMe](https://github.com/wkentaro/labelme)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fe1192-dba2-4e6b-bcad-d8844c6aba70",
   "metadata": {},
   "source": [
    "### Roboflow API\n",
    "To make use of a pre-trained model on Roboflow, you need to establish a connection with their cloud-based inference service. This requires:\n",
    "\n",
    "1) Insert your specific project API key to authenticate\n",
    "2) Once connected, you'll be able to view prediction results\n",
    "\n",
    "In this example, we connect with our own pre-trained YOLO model which has been trained on custom-labeled traffic signs dataset. The model is capable of detecting and classifying four different categories of traffic signs.   ![](https://www.mdpi.com/applsci/applsci-13-04793/article_deploy/html/images/applsci-13-04793-g001.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288c8856-50bb-4567-acf5-74cd2620c513",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install inference_sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f033bb0e-f893-4c6d-91a4-19e584f32dea",
   "metadata": {},
   "source": [
    "[inference_sdk](https://inference.roboflow.com/inference_helpers/inference_sdk/#getting-server-info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22a2cfe-7b4e-40f1-9940-1dca44bdd664",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import the inference-sdk\n",
    "from inference_sdk import InferenceHTTPClient\n",
    "\n",
    "# initialize the client\n",
    "CLIENT = InferenceHTTPClient(\n",
    "    api_url=\"https://detect.roboflow.com\",\n",
    "    api_key=\"your_api_key_here\"  # Replace with your actual API key\n",
    ")\n",
    "\n",
    "# infer on a local image\n",
    "result1 = CLIENT.infer(\"11502.png\", model_id=\"traffic-signs-2-k9pcv/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7a0173-1a93-4173-8909-6181f15a2698",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30408f75",
   "metadata": {},
   "source": [
    "**What is displayed?**\n",
    "\n",
    "The inference result shows that an image of size 62x93 pixels was analyzed in 0.13 seconds, and one object was detected. The detected object is of class \"Priority\" with a confidence score of 71.15%. The object's bounding box is centered at (31, 46) with a width of 56 and height of 72. The result is associated with an inference ID and a detection ID for tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fac8ba-9fca-40e7-b066-4a5e56052a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image using OpenCV\n",
    "image = cv2.imread('11502.png')\n",
    "if image is None:\n",
    "    raise FileNotFoundError(\"Image not found.\")\n",
    "\n",
    "# Resize image (assuming result1 contains 'image' dimensions)\n",
    "image_resized = cv2.resize(image, (result1['image']['width'], result1['image']['height']))\n",
    "\n",
    "# Convert BGR image to RGB for matplotlib\n",
    "image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Create a figure and axis for matplotlib\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Display the image using matplotlib\n",
    "ax.imshow(image_rgb)\n",
    "\n",
    "# Draw bounding boxes and text on the image using matplotlib\n",
    "for index, pred in enumerate(result1['predictions']):\n",
    "    xmin, ymin, xmax, ymax = map(int, [pred['x'] - pred['width'] // 2, pred['y'] - pred['height'] // 2,\n",
    "                                      pred['x'] + pred['width'] // 2, pred['y'] + pred['height'] // 2])\n",
    "\n",
    "    # Add bounding box using matplotlib patches\n",
    "    ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, \n",
    "                               edgecolor='yellow', fill=False, linewidth=3))\n",
    "\n",
    "    # Add text above the bounding box (adjust ymin to place it above)\n",
    "    label = f\"{pred['class']}: {pred['confidence']:.2f}\"\n",
    "    ax.text(xmin, ymin-1 , label, color='yellow', fontsize=12, backgroundcolor='none')\n",
    "\n",
    "# Hide axes for a cleaner view\n",
    "ax.axis('off')\n",
    "\n",
    "# Show the image with bounding boxes and text using matplotlib\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5c4c1e-395c-4909-ba0d-76d754665b9d",
   "metadata": {},
   "source": [
    "### Portfolio assignment 22\n",
    "30 min: Let Yolov8 do predictions on your own choosen dataset.\n",
    "- Load your data in and have a quick look\n",
    "- Let YOLO predict from one of your images the objects\n",
    "- Evaluate performance of your YOLO model by using the Confusion Matrix and Intersection of Union."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e67d6a-b03b-4942-8700-b4d48d913280",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/0v1CGNV.png)<br>\n",
    "Assumption: ...<br>\n",
    "Finding: ...<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad09049-d685-40d7-acec-35329b519094",
   "metadata": {},
   "source": [
    "##### Extra info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344c88ba-96b5-4557-9699-0da5cb2ab009",
   "metadata": {},
   "source": [
    "When you dowloaded a dataset from the internet often there is a yaml file. \n",
    "YAML (Yet Another Markup Language) is a **human-readable** format used for configuration files. It is simple, supports key-value pairs, and is widely used in machine learning setups.  \n",
    "\n",
    "YOLO uses YAML to define:  \n",
    "- **Dataset paths** (train/val images)  \n",
    "- **Number of classes** (`nc`)  \n",
    "- **Class names** (`names`)  \n",
    "\n",
    "It is preferred because it is lightweight, easy to edit, and Python-friendly.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
