{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92edfd45",
      "metadata": {},
      "source": [
        "# Portfolio\n",
        "\n",
        "This portfolio showcases the assignments completed for the Data Science learning unit. Each assignment demonstrates the application of data science techniques and tools to solve real-world problems.\n",
        "\n",
        "#### Datasets Used:\n",
        "- **[Gym Members Dataset](https://www.kaggle.com/datasets/valakhorasani/gym-members-exercise-dataset):** Contains information about gym members, their exercise habits, and demographics.\n",
        "- **[Fraud Detection Dataset](https://www.kaggle.com/datasets/aryan208/financial-transactions-dataset-for-fraud-detection):** Includes financial transaction data for identifying fraudulent activities.\n",
        "- **[Calorie Expenditure Dataset](https://www.kaggle.com/competitions/playground-series-s5e5/data):** Provides data on calorie expenditure during various activities."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4072bc2d",
      "metadata": {},
      "source": [
        "# Assignment 1: What is Data Science?\n",
        "\n",
        "Data Science is a multidisciplinary field that uses various (scientific) methodes to extract insights and knowledge from structured and unstructured data. It involves the collection, cleaning, analysis, and visualization of data to inform decision-making and drive business strategies.\n",
        "\n",
        "#### What does a Data Scientist do?\n",
        "\n",
        "A Data Scientist is a person who responsible for collecting, analyzing and orginizing data to extract useful information. Then communicating the findings to people who can use it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "446c73b0",
      "metadata": {},
      "source": [
        "# Assignment 2: Popular Tools for Data Science\n",
        "\n",
        "Some of the popular tools used in Data Science include:\n",
        "\n",
        "- **Programming Languages**: Python, R, SQL\n",
        "- **Data Manipulation and Analysis**: Pandas, NumPy\n",
        "- **Data Visualization**: Matplotlib, Seaborn, Tableau\n",
        "- **Machine Learning**: Scikit-learn, TensorFlow, PyTorch\n",
        "- **Big Data Tools**: Apache Spark, Hadoop\n",
        "- **Integrated Development Environments (IDEs)**: Jupyter Notebook, RStudio"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f38de00",
      "metadata": {},
      "source": [
        "# Assignment 3: Univariate Analisys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b276f06",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "penguins3 = sns.load_dataset(\"penguins\")\n",
        "penguins3.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d220961",
      "metadata": {},
      "source": [
        "#### Univariate Analysis on categorical data on the penguins dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ccac522",
      "metadata": {},
      "outputs": [],
      "source": [
        "penguins3['island'].value_counts(dropna=True).plot(kind='bar')\n",
        "# penguins['sex'].value_counts(dropna=True).plot(kind='bar')\n",
        "# penguins['species'].value_counts(dropna=True).plot(kind='bar')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec41d1d7",
      "metadata": {},
      "source": [
        "# Assignment 4 - Finding a dataset\n",
        "\n",
        "I have looked on Kaggle for a dataset to use for the univariate analysis. I found a dataset about gym members and their exercise habits. The dataset contains information about gym members, their exercise habits, and their demographics. The dataset includes columns such as age, gender, height, weight, and exercise frequency. The dataset is suitable for univariate analysis as it contains both categorical and numerical variables.\n",
        "\n",
        "The dataset can be found here: [https://www.kaggle.com/datasets/valakhorasani/gym-members-exercise-dataset](https://www.kaggle.com/datasets/valakhorasani/gym-members-exercise-dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ded903c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "gym4 = pd.read_csv(\"Datasets\\\\gym_members_exercise_tracking.csv\")\n",
        "gym4.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcfeeac8",
      "metadata": {},
      "source": [
        "# Assignment 5 - Univariate Analysis\n",
        "\n",
        "For this assignment I will be using the lifeExpectancyAtBirth dataset from the WHO. The dataset contains information about life expectancy at birth for different countries and regions. The dataset includes columns such as country, region, year, and life expectancy. The dataset is suitable for univariate analysis as it contains both categorical and numerical variables.\n",
        "\n",
        "The dataset can be found here: [https://www.kaggle.com/datasets/utkarshxy/who-worldhealth-statistics-2020-complete](https://www.kaggle.com/datasets/utkarshxy/who-worldhealth-statistics-2020-complete)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "189d937b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "life_expectancy5 = pd.read_csv(\"Datasets\\\\lifeExpectancyAtBirth.csv\")\n",
        "life_expectancy5.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4d98e96",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter the dataframe for 2019 and 'Both sexes'\n",
        "expectancy_2019 = life_expectancy5[\n",
        "    (life_expectancy5['Period'] == 2019) & \n",
        "    (life_expectancy5['Dim1'] == 'Both sexes')\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ee13d0d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter the dataframe for 2019 and 'Both sexes'\n",
        "print(\"Five countries with the highest life expectancy:\")\n",
        "print(expectancy_2019.nlargest(5, 'First Tooltip')[['Location', 'First Tooltip']])\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Five countries with the lowest life expectancy:\")\n",
        "print(expectancy_2019.nsmallest(5, 'First Tooltip')[['Location', 'First Tooltip']])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a67850fc",
      "metadata": {},
      "source": [
        "# Assignment 6\n",
        "\n",
        "For this assignment I will be using the dataset I chose in assignment 4. The dataset contains information about gym members, their exercise habits, and their demographics. I wil be performing univariate analysis on 2 categorical collums and 2 numerical columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4798715f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "gym6 = pd.read_csv(\"Datasets\\\\gym_members_exercise_tracking.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38bba276",
      "metadata": {},
      "source": [
        "I will be using the following columns for the univariate analysis:\n",
        "Gender and WorkoutType for the categorical columns and Age and Weight for the numerical columns.\n",
        "\n",
        "The dataset can be found here: [https://www.kaggle.com/datasets/valakhorasani/gym-members-exercise-dataset](https://www.kaggle.com/datasets/valakhorasani/gym-members-exercise-dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77613522",
      "metadata": {},
      "outputs": [],
      "source": [
        "gym6['Gender'].value_counts(dropna=True).plot(kind='bar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "799058fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "gym6['Workout_Type'].value_counts(dropna=True).plot(kind='bar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd9c8c28",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "gym6['Weight (kg)'].plot(kind='box', vert=True, patch_artist=True, boxprops=dict(facecolor='lightblue'))\n",
        "plt.title('Distribution of Weight (kg)', fontsize=14)\n",
        "plt.xlabel('Weight (kg)', fontsize=12)\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb89cac4",
      "metadata": {},
      "outputs": [],
      "source": [
        "gym6['Workout_Frequency (days/week)'].value_counts().plot(kind='pie', autopct='%1.1f%%', ylabel='', title='Workout Frequency (days/week)')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1af6891",
      "metadata": {},
      "source": [
        "# Assignment 7 - Distributions\n",
        "\n",
        "I will analyse the histograms of at least two numerical columns from the dataset I chose in portfolio assignment 4. I will observe the distribution of the data and identify whether it resembles a uniform, normal, or another type of distribution.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e44f5d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "gym7 = pd.read_csv(\"Datasets\\\\gym_members_exercise_tracking.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "188e8fa1",
      "metadata": {},
      "outputs": [],
      "source": [
        "gym7['BMI'].plot(kind='hist', bins=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2526a77b",
      "metadata": {},
      "source": [
        "The above analysis was done on the BMI variable. The histogram of the BMI variable shows that the data is Log-Normal distributed. The histogram is skewed to the right, indicating that there are a few individuals with very high BMI values. The majority of the individuals have a BMI value between 18 and 28, which is considered normal weight."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e92f12f",
      "metadata": {},
      "outputs": [],
      "source": [
        "gym7['Session_Duration (hours)'].plot(kind='hist', bins=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c374314",
      "metadata": {},
      "source": [
        "The above analysis was done using the workout session duration. I have observed that the data is Tukey-Lambda distributed. The histogram is in the shape of a bell curve, indicating that the majority of the individuals have a workout session duration between 60 and 90 minutes. There are a still some individuals with a workout session duration of more than 90 minutes and less than 30 minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60f834c8",
      "metadata": {},
      "source": [
        "# Assignment 8 - Confidence Intervals\n",
        "\n",
        "In this assignment I will calculate the 90%, 95%, 99%, and 99.99% confidence intervals for at least two numerical columns in the dataset I chose in portfolio assignment 4. I will observe the impact of the confidence level on the interval. Next, I will calculate the 95% confidence interval again but using only the first 10% of the rows. I will compare this interval to the previous 95% confidence interval I calculated and analyze the impact of having less data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b023656",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import scipy.stats as st\n",
        "gym8 = pd.read_csv(\"Datasets\\\\gym_members_exercise_tracking.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9da0eda",
      "metadata": {},
      "source": [
        "Below is the code I used to calculate the confidence intervals for the Calories_Burned variable. I made a function that takes the dataset as input and prints the confidence interval. I used the t-distribution to calculate the confidence intervals. Why I used the t-distribution? Because the sample size is small and the population standard deviation is unknown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6017e384",
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_confidence(dataset : pd.DataFrame):\n",
        "  confidence_levels = [0.90, 0.95, 0.99, 0.9999]\n",
        "  for confidence in confidence_levels:\n",
        "    interval = st.t.interval(\n",
        "      confidence, \n",
        "      len(dataset) - 1, \n",
        "      loc=dataset['Calories_Burned'].mean(), \n",
        "      scale=st.sem(dataset['Calories_Burned'])\n",
        "    )\n",
        "    print(f\"{float(confidence * 100)}% Confidence Interval: {interval}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10cd7ca7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print confidence intervals for the entire dataset\n",
        "print_confidence(gym8)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d7234b8",
      "metadata": {},
      "source": [
        "Next, I will use the same function to calculate the confidence intervals for only the first 10% of the rows. I will compare this interval to the previous 95% confidence interval I calculated and analyze the impact of having less data and why this is important."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ea02955",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the first 10% of the dataset\n",
        "first_10_percent_count = int(len(gym8) * 0.1)\n",
        "first_10_percent_data = gym8.iloc[:first_10_percent_count]\n",
        "\n",
        "# Print confidence intervals for the first 10% of the dataset\n",
        "print_confidence(first_10_percent_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8c1bbdd",
      "metadata": {},
      "source": [
        "With **less data**, we have less information about the overall population. This makes our estimates of the mean more uncertain which is reflected in a **wider confidence interval**. To still capture the true population mean with the same level of confidence (e.g. 95%), we need to account for this uncertainty by **expanding the range**. \n",
        "\n",
        "Alternatively, if we had a dataset that was **sorted** in any way, we would have to be careful about the first 10% of the rows. For example, if the dataset was sorted by age, the first 10% of the rows would only contain individuals aged 18-25. This would **not be a representative sample** of the overall population and would lead to **inaccurate estimates** of the confidence intervals."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bd5320f",
      "metadata": {},
      "source": [
        "# Assignment 9 - Numerical Bivariate Analysis\n",
        "\n",
        "I performed a bivariate analysis on the columns with numerical data in the penguins dataset. I used `corr()` on the DataFrame to calculate all the correlations. The correlation table was displayed with colors using the code example provided earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f649e6ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "penguins9 = sns.load_dataset(\"penguins\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a21d2f3a",
      "metadata": {},
      "outputs": [],
      "source": [
        "corrola = penguins9.corr(numeric_only=True)\n",
        "corrola.style.background_gradient(cmap='coolwarm', axis=None).format(precision=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef26cd4b",
      "metadata": {},
      "source": [
        "# Assignment 10\n",
        "I performed a bivariate analysis on the `BMI` and `Calories_Burned` columns from the dataset I chose in portfolio assignment 4. The Pearson correlation coefficient was calculated, and a scatter plot was created to visualize the relationship between these two variables. The correlation coefficient indicates the strength and direction of the linear relationship, while the scatter plot provides a visual representation of the data points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca9bb6c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "gym10 = pd.read_csv(\"Datasets\\\\gym_members_exercise_tracking.csv\")\n",
        "correlation = gym10.corr(numeric_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a917b73b",
      "metadata": {},
      "outputs": [],
      "source": [
        "correlation.style.background_gradient(cmap='coolwarm', axis=None).format(precision=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "542c118c",
      "metadata": {},
      "outputs": [],
      "source": [
        "gym10.plot(kind='scatter', x='Calories_Burned', y='Session_Duration (hours)', title='Calories Burned vs Session Duration')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3fa7a46",
      "metadata": {},
      "outputs": [],
      "source": [
        "gym10.plot(kind='scatter', x='Fat_Percentage', y='Calories_Burned', title='Fat Percentage vs Calories Burned')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08f3ee38",
      "metadata": {},
      "source": [
        "# Assignment 11 - Numerical vs Categorical Analysis\n",
        "\n",
        "I am going to do a Numerical VS Categorical bivariate analysis on the penguins dataset. I have chosen the \"species\" categorical collum. I will use `.groupby('<columnname>').mean()` to examine the means of the numerical columns and see if there are differences between the categories. Next, I will use the seaborn barplot to visualize the mean and confidence intervals for each of the numerical columns (`bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, `body_mass_g`). I have done this for the `species` column, the `sex` column and the `island` column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7348c72",
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "penguins11 = sns.load_dataset(\"penguins\")\n",
        "penguins11.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c10cda6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_barplot(data, x, y, title):\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  sns.barplot(data=data, x=x, y=y)\n",
        "  plt.title(title, fontsize=16)\n",
        "  plt.xlabel(x, fontsize=14)\n",
        "  plt.ylabel(y, fontsize=14)\n",
        "  plt.xticks(rotation=45)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0120a04c",
      "metadata": {},
      "outputs": [],
      "source": [
        "penguins11.groupby('species').mean(numeric_only=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2520a27",
      "metadata": {},
      "outputs": [],
      "source": [
        "penguins11.groupby('island').mean(numeric_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1754f470",
      "metadata": {},
      "outputs": [],
      "source": [
        "penguins11.groupby('sex').mean(numeric_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29ce3bf8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loop through the numerical columns and create bar plots grouped by 'species'.\n",
        "for column in ['bill_length_mm', 'body_mass_g', 'bill_depth_mm', 'flipper_length_mm']:\n",
        "  display_barplot(data=penguins11, x='species', y=column, title=f'{column} by Species')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db624f39",
      "metadata": {},
      "source": [
        "SCIENCE CONCLUSION SPECIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0cdd9b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loop through the numerical columns and create bar plots grouped by 'island'.\n",
        "for column in ['bill_length_mm', 'body_mass_g', 'bill_depth_mm', 'flipper_length_mm']:\n",
        "  display_barplot(data=penguins11, x='island', y=column, title=f'{column} by Island')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37b126e5",
      "metadata": {},
      "source": [
        "SCIENCE CONCLUSION ISLAND"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5240c28",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loop through the numerical columns and create bar plots grouped by 'sex'.\n",
        "for column in ['bill_length_mm', 'body_mass_g', 'bill_depth_mm', 'flipper_length_mm']:\n",
        "  display_barplot(data=penguins11,x='sex',y=column,title=f'{column} by Sex')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27289084",
      "metadata": {},
      "source": [
        "SCIENCE CONCLUSION SEX"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d52f8805",
      "metadata": {},
      "source": [
        "# Assignment 12\n",
        "\n",
        "For this assignment, I performed a bivariate analysis on three combinations of numerical and categorical columns from the gym members dataset chosen in portfolio assignment 4. The combinations analyzed are:\n",
        "\n",
        "1. **Workout_Type** (categorical) vs **Calories_Burned** (numerical)\n",
        "2. **Gender** (categorical) vs **BMI** (numerical)\n",
        "3. **Workout_Frequency (days/week)** (categorical) vs **Session_Duration (hours)** (numerical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8319751b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "gym12 = pd.read_csv(\"Datasets\\\\gym_members_exercise_tracking.csv\")\n",
        "gym12.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8732df98",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_contingency(dataset, column1, column2):\n",
        "  return dataset.groupby(column1)[column2].mean().reset_index().style.set_table_styles([\n",
        "  {'selector': 'caption', 'props': [('font-size', '16px'), ('text-align', 'center')]}])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59f4e91b",
      "metadata": {},
      "source": [
        "### Calories Burned per Workout Type\n",
        "\n",
        "The dataset provides insights into the average calories burned for different workout types. By analyzing this data, we can identify which workout types are more effective in terms of calorie expenditure. This information is valuable for individuals aiming to optimize their fitness routines based on their goals, such as weight loss or endurance building."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1a28748",
      "metadata": {},
      "outputs": [],
      "source": [
        "create_contingency(gym12 ,\"Workout_Type\", \"Calories_Burned\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06e2146a",
      "metadata": {},
      "outputs": [],
      "source": [
        "display_barplot(data=gym12, x='Workout_Type', y='Calories_Burned', title='Calories Burned by Workout Type')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ede7fc0",
      "metadata": {},
      "source": [
        "In the graph above, you can see that the average calories burned per workout type do not differ much. They all fall within each other's 95% confidence interval. This means that there is no significant difference between the average calories burned per workout type. This could be because the dataset is not large enough, that the workout types do not differ much in intensity, or that the individuals in the dataset are not representative of the overall population."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b49d8ea",
      "metadata": {},
      "source": [
        "### BMI per Gender\n",
        "\n",
        "The dataset provides insights into the average BMI for different genders. By analyzing this data, we can identify any significant differences in BMI between males and females. This information is valuable for understanding the impact of the gender on body composition and health."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d02a527b",
      "metadata": {},
      "outputs": [],
      "source": [
        "create_contingency(gym12, \"Gender\", \"BMI\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db539162",
      "metadata": {},
      "outputs": [],
      "source": [
        "display_barplot(data=gym12, x='Gender', y='BMI', title='BMI by Gender')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af33a2b1",
      "metadata": {},
      "source": [
        "The graph above shows that the average BMI does differ per gender. The average BMI of men is higher than that of women. This may be because men have more muscle mass on average than women. Women also have more fat mass on average than men. This is a normal difference between the genders."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a65cd3f",
      "metadata": {},
      "source": [
        "### Session_Duration (hours) per Workout_Frequency (days/week) \n",
        "\n",
        "The dataset provides insights into the average session duration for different workout frequencies. By analyzing this data, we can identify how the frequency of workouts affects the duration of each session. This information is valuable for individuals looking to optimize their workout schedules and ensure they are getting the most out of their gym time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd2ff4ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "gym12.groupby(\"Workout_Frequency (days/week)\")[\"Session_Duration (hours)\"].mean().reset_index().style.set_table_styles([\n",
        "  {'selector': 'caption', 'props': [('font-size', '16px'), ('text-align', 'center')]}])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98518efc",
      "metadata": {},
      "outputs": [],
      "source": [
        "display_barplot(data=gym12, x='Workout_Frequency (days/week)', y='Session_Duration (hours)',\n",
        "                title='Session Duration by Workout Frequency')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0230fd80",
      "metadata": {},
      "source": [
        "In the graph above, you can see that the average session duration does differ per workout frequency. The average session duration is higher for individuals who work out more frequently. This may be because individuals who work out more frequently are more experienced and can handle longer sessions. It may also be because individuals who work out more frequently have more time to spend at the gym or they enjoy working out more."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb51e1e1",
      "metadata": {},
      "source": [
        "# Assignment 13 - Categorical Bivariate Analysis\n",
        "\n",
        "I performed a bivariate analysis on the penguins dataset for the combinations of species vs sex and island vs sex. In this analysis, I first considered whether a correlation between the columns was expected. Then, I created a contingency table to observe the ratios between categories and visualized these ratios using a bar plot. Finally, I conducted a chi-squared test to determine the statistical significance of the correlation, interpreted the results, and calculated the likelihood of a correlation between the columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b428fadb",
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "penguins13 = sns.load_dataset(\"penguins\")\n",
        "penguins13.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1c2d069",
      "metadata": {},
      "source": [
        "#### My hypothesis:\n",
        "I do not expect a correlation between the species and sex columns. The species of the penguins are not related to how many male or female penguins there are. </br>\n",
        "I do not expect a correlation between the island and sex columns. The island of the penguins are not related to how many males or female penguins there are. </br>\n",
        "I do expect a correlation between the species and island columns. The species of the penguins are related to which island they live on. </br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6e2e12e",
      "metadata": {},
      "source": [
        "I am going to make use of a few functions to make the code more readable and reusable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "587bb444",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_contingency_table(dataset, column1, column2):\n",
        "    return dataset.groupby([column1, column2]).size().unstack(column1, fill_value=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50d07458",
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_cat_vs_cat_barplot(dataset, column1, column2):\n",
        "  dataset.groupby([column1, column2]).size().unstack(column1, fill_value=0).plot(kind='bar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edd15b65",
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "def check_cat_vs_cat_correlation(dataset, column1, column2):\n",
        "    contingency_table = create_contingency_table(dataset, column1, column2)\n",
        "    chi2 = chi2_contingency(contingency_table)\n",
        "    print(chi2)\n",
        "    p_value = chi2[1]\n",
        "    odds_of_correlation = 1 - p_value\n",
        "    print(f\"The odds of a correlation between {column1} and {column2} is {odds_of_correlation * 100}% (Based on a p value of {p_value}).\")\n",
        "    print(\"This percentage needs to be at least 95% for a significant correlation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f309bf49",
      "metadata": {},
      "source": [
        "### Species vs Sex\n",
        "\n",
        "First, I will create a contingency table to observe the ratios between the categories so I can see if there is a correlation between the species and sex columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70517cff",
      "metadata": {},
      "outputs": [],
      "source": [
        "create_contingency_table(penguins13, 'species','sex')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b93a527",
      "metadata": {},
      "source": [
        "The contingency table above shows the number of penguins per species and sex. The table shows that there are nearly equal numbers of males and females for each species. The only exception is the Gentoo species, where there are slightly more males than females. This is not a significant difference. Next I will use Chi-squared test to determine if there is a significant correlation between the species and sex columns. The null hypothesis is that there is no correlation because of the contengency table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c560b703",
      "metadata": {},
      "outputs": [],
      "source": [
        "check_cat_vs_cat_correlation(penguins13, 'species','sex')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ffbab84",
      "metadata": {},
      "source": [
        "As I expected, the p-value is way higher than 0.05. This means that there is no significant correlation between the species and sex columns. In this case there is a 2.4% change of correlation. This is also visable in a bar plot. I will put that underneath this text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "564e72f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "make_cat_vs_cat_barplot(penguins13, 'species','sex')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "392fb367",
      "metadata": {},
      "source": [
        "### Island vs Sex\n",
        "\n",
        "First, I will create a contingency table to observe the ratios between the categories so I can see if there is a correlation between the sex and island columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84f96375",
      "metadata": {},
      "outputs": [],
      "source": [
        "create_contingency_table(penguins13, 'island','sex')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07f9e1ac",
      "metadata": {},
      "source": [
        "The contingency table above shows the number of penguins per island and sex. The table shows that there are nearly equal numbers of males and females on each island. I do not expect a correlation between the island and sex columns. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b105168b",
      "metadata": {},
      "outputs": [],
      "source": [
        "check_cat_vs_cat_correlation(penguins13, 'sex','island')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed24d2dc",
      "metadata": {},
      "source": [
        "As I expected, the p-value is way higher than 0.05. This means that there is no significant correlation between the island and sex columns. In this case there is a 2.8% change of correlation. This is also visable in a bar plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "139bff97",
      "metadata": {},
      "outputs": [],
      "source": [
        "make_cat_vs_cat_barplot(penguins13, 'sex', \"island\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca05a13b",
      "metadata": {},
      "source": [
        "# Assignment 14\n",
        "\n",
        "I performed a catagorical bivariate analysis on my dataset that I chose in assignment 4. In this analysis, I first consider whether a correlation between the columns is expected. Then, I create a contingency table to observe the ratios between categories and visualized these ratios using a bar plot. Finally, I conducted a chi-squared test to determine the statistical significance of the correlation, interpreted the results, and calculated the likelihood of a correlation between the columns.\n",
        "\n",
        "I am going to make use of a few functions to make the code more readable and reusable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "facd60c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "gym14 = pd.read_csv(\"Datasets\\\\gym_members_exercise_tracking.csv\")\n",
        "gym14.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a05f703e",
      "metadata": {},
      "source": [
        "#### My hypothesis:\n",
        "I do expect a correlation between the experience and workout frequency columns. The more experienced individuals are, the more frequently they work out. </br>\n",
        "I do not expect a correlation between the workout type and workout frequency columns. The workout type does not determine how frequently individuals work out. </br>\n",
        "I do not expect a correlation between the workout type and experience columns. The workout type does not determine how experienced individuals are. </br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6be43579",
      "metadata": {},
      "source": [
        "#### Experience vs Workout Frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2af069fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "create_contingency_table(gym14, 'Experience_Level', 'Workout_Frequency (days/week)')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d59a1ee",
      "metadata": {},
      "source": [
        "In the contingency table above, I can already see that there is a correlation between the experience and workout frequency columns. The table shows that the more experienced individuals are, the more frequently they work out. This is also visible in the bar plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d668cd86",
      "metadata": {},
      "outputs": [],
      "source": [
        "make_cat_vs_cat_barplot(gym14, 'Experience_Level', 'Workout_Frequency (days/week)')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf74326c",
      "metadata": {},
      "source": [
        "I will now use Chi-squared test to determine if there is a significant correlation between the experience and workout frequency columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2647b768",
      "metadata": {},
      "outputs": [],
      "source": [
        "check_cat_vs_cat_correlation(gym14, 'Experience_Level', 'Workout_Frequency (days/week)')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30a1c2ef",
      "metadata": {},
      "source": [
        "#### Workout Type vs Workout Frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22fd988e",
      "metadata": {},
      "outputs": [],
      "source": [
        "create_contingency_table(gym14, 'Workout_Type', 'Workout_Frequency (days/week)')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be0d0321",
      "metadata": {},
      "source": [
        "In the contingency table above, I can't clearly see a correlation. I will make a bar plot to make it more visual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1695f9f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "make_cat_vs_cat_barplot(gym14, 'Workout_Type', 'Workout_Frequency (days/week)')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acf9f3f9",
      "metadata": {},
      "source": [
        "In the bar graph I clearly saw that there is probably no correlation. I will now use Chi-squared test to make sure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba291039",
      "metadata": {},
      "outputs": [],
      "source": [
        "check_cat_vs_cat_correlation(gym14, 'Workout_Type', 'Workout_Frequency (days/week)')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df38d549",
      "metadata": {},
      "source": [
        "As expected, the p-value is 0.76... (which is way higher than 0.05). This means that there is no significant correlation between the workout type and workout frequency columns. In this case there is a 23.6% change of correlation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "458f1473",
      "metadata": {},
      "source": [
        "#### Experience vs Workout Type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3103bcf1",
      "metadata": {},
      "outputs": [],
      "source": [
        "create_contingency_table(gym14, 'Experience_Level', 'Workout_Type')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b26faec",
      "metadata": {},
      "source": [
        "In the contingency table above, I cannot clearly see a correlation between the experience and workout type columns. I will make a bar plot to make it more visual and see if there is a correlation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "666be3be",
      "metadata": {},
      "outputs": [],
      "source": [
        "make_cat_vs_cat_barplot(gym14, 'Experience_Level', 'Workout_Type')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2a08bf1",
      "metadata": {},
      "source": [
        "In the bar graph I clearly saw that there is probably no correlation. I will run the Chi-squared test to confirm this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48c63ab1",
      "metadata": {},
      "outputs": [],
      "source": [
        "check_cat_vs_cat_correlation(gym14, 'Experience_Level', 'Workout_Type')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "533e1519",
      "metadata": {},
      "source": [
        "And the Chi-squared test confirms that there is no significant correlation between the experience and workout type columns. In this case there is a 55.3% change of correlation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2c99101",
      "metadata": {},
      "source": [
        "# Assignment 15 - Classification\n",
        "\n",
        "In this assignment, I am going to train a decision tree to predict the species of a penguin based on their caracteristics. I will use the penguins dataset for this assignment. The dataset contains information about the species, island, bill length, bill depth, flipper length, and body mass of penguins. I will use the `DecisionTreeClassifier` from the `sklearn.tree` module to train the decision tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7da90ce9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import graphviz as gv\n",
        "import sklearn.tree as tree\n",
        "import sklearn.model_selection as msn\n",
        "\n",
        "penguins15 = sns.load_dataset(\"penguins\")\n",
        "penguins15.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2081d79b",
      "metadata": {},
      "source": [
        "I am going to train the model using the DecisionTreeClassifier from the sklearn.tree module. The code below shows how. I am setting max depth to 5 as this is a good starting point. I will also set the random state to 42 to ensure that the results are reproducible. As I researched earlier, the sex and island columns are not useful for the prediction. I will drop these columns from the dataset I am going to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec410826",
      "metadata": {},
      "outputs": [],
      "source": [
        "features15 = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
        "dt15 = tree.DecisionTreeClassifier(max_depth=5)\n",
        "dt15.fit(penguins15[features15], penguins15[\"species\"]) # First the columns I am using to predict, then the target column."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1734fe72",
      "metadata": {},
      "source": [
        "The code above has outputted the decision tree. In order to visualize the decision tree, I will use a custom function shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7f2b373",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_tree_classification(model, features, class_names):\n",
        "  return gv.Source(tree.export_graphviz(model, out_file=None, feature_names=features, class_names=class_names, filled=True, rounded=True, special_characters=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0db39af6",
      "metadata": {},
      "source": [
        "Using this custom function, I can visualize the decision tree:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9aa9f2d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_tree_classification(dt15, features15, np.sort(penguins15.species.unique()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccf33690",
      "metadata": {},
      "source": [
        "Now I will go on to validate the model. I will use `dt15.predict` to use the model to predict the species of the penguins that are in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d39e115d",
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = dt15.predict(penguins15[features15])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68ee47f1",
      "metadata": {},
      "source": [
        "Using the code below, I can see the predictions that the model made."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b633c09f",
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f979dcb",
      "metadata": {},
      "source": [
        "Using another custom function, I can calculate the accuracy of the model. The accuracy is the percentage of correct predictions made by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11457bd2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_accuracy(predictions, actuals):\n",
        "    if(len(predictions) != len(actuals)):\n",
        "        raise Exception(\"The amount of predictions did not equal the amount of actuals\")\n",
        "    return (predictions == actuals).sum() / len(actuals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab5b2c72",
      "metadata": {},
      "outputs": [],
      "source": [
        "calculate_accuracy(predictions, penguins15.species)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d62138a6",
      "metadata": {},
      "source": [
        "As you can see, the accuracy of the model is 99.41%. This is because the dataset is relatively small and the decision tree is probably `overfitting` the data. I will now use a `70/30 split` to train and test the model. I will use the `train_test_split` function from the `sklearn.model_selection` module to do this. I will also set the random state to 42 to ensure that the results are reproducible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b14335f",
      "metadata": {},
      "outputs": [],
      "source": [
        "penguins15_train, penguins15_test = msn.train_test_split(penguins15, test_size=0.3, random_state=42, stratify=penguins15['species'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88e4b9e2",
      "metadata": {},
      "source": [
        "Then I will remake the decision tree and visualize it again. I will use the same custom function as before to do this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3c66a82",
      "metadata": {},
      "outputs": [],
      "source": [
        "features15 = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
        "dt15 = tree.DecisionTreeClassifier(max_depth=5)\n",
        "dt15.fit(penguins15_train[features15], penguins15_train[\"species\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc76f622",
      "metadata": {},
      "source": [
        "Now I will use the trained decision tree to predict the species of a penguin based on their characteristics. I will use the test set to evaluate the performance of the model. Also, I will use the train set to evaluate if the model is overfitting the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ecef54d",
      "metadata": {},
      "outputs": [],
      "source": [
        "predictionsOnTrainset15 = dt15.predict(penguins15_train[features15])\n",
        "predictionsOnTestset15 = dt15.predict(penguins15_test[features15])\n",
        "\n",
        "accuracyTrain = calculate_accuracy(predictionsOnTrainset15, penguins15_train.species)\n",
        "accuracyTest = calculate_accuracy(predictionsOnTestset15, penguins15_test.species)\n",
        "\n",
        "print(\"Accuracy on training set \" + str(accuracyTrain))\n",
        "print(\"Accuracy on test set \" + str(accuracyTest))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b72354b",
      "metadata": {},
      "source": [
        "An unexpected result of the model is that the accuracy of the model on the test set is `97%`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18c62742",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and evaluate a decision tree with max_depth=6\n",
        "dt15 = tree.DecisionTreeClassifier(max_depth=6)\n",
        "dt15.fit(penguins15_train[features15], penguins15_train[\"species\"])\n",
        "train_predictions = dt15.predict(penguins15_train[features15])\n",
        "test_predictions = dt15.predict(penguins15_test[features15])\n",
        "train_accuracy = (train_predictions == penguins15_train[\"species\"]).mean()\n",
        "test_accuracy = (test_predictions == penguins15_test[\"species\"]).mean()\n",
        "print(f\"Max Depth: 6 - Train Accuracy: {train_accuracy:.2%}, Test Accuracy: {test_accuracy:.2%}\")\n",
        "\n",
        "# Train and evaluate a decision tree with max_depth=3\n",
        "dt15 = tree.DecisionTreeClassifier(max_depth=3)\n",
        "dt15.fit(penguins15_train[features15], penguins15_train[\"species\"])\n",
        "train_predictions = dt15.predict(penguins15_train[features15])\n",
        "test_predictions = dt15.predict(penguins15_test[features15])\n",
        "train_accuracy = (train_predictions == penguins15_train[\"species\"]).mean()\n",
        "test_accuracy = (test_predictions == penguins15_test[\"species\"]).mean()\n",
        "print(f\"Max Depth: 3 - Train Accuracy: {train_accuracy:.2%}, Test Accuracy: {test_accuracy:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d4dc27c",
      "metadata": {},
      "source": [
        "Setting the max depth to 6 increased the accuracy of the model on the test set. Setting it to 3 decreased the accuracy both on the train and test set."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63f377c7",
      "metadata": {},
      "source": [
        "# Assignment 16\n",
        "\n",
        "In this assignment, I am going to train a decision tree to predict a fraudulent transaction. For this assignment, I will use a new dataset that I found on Kaggle. The dataset contains information about credit card transactions and whether they are fraudulent or not. I will use the `DecisionTreeClassifier` from the `sklearn.tree` module to train the decision tree. The dataset can be found here: [https://www.kaggle.com/datasets/aryan208/financial-transactions-dataset-for-fraud-detection/](https://www.kaggle.com/datasets/aryan208/financial-transactions-dataset-for-fraud-detection/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40c68c96",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import hashlib as hl\n",
        "import graphviz as gv\n",
        "import sklearn.tree as tree\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.model_selection as msn\n",
        "\n",
        "fraud16 = pd.read_csv(\"Datasets\\\\financial_fraud_detection_dataset.csv\")\n",
        "fraud16.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe85df1e",
      "metadata": {},
      "source": [
        "I am first going to drop the a few columns from the dataset. These columns are not useful for the prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "289c76c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "fraud16.drop(columns=[\"transaction_id\"], inplace=True)\n",
        "fraud16.drop(columns=[\"fraud_type\"], inplace=True)\n",
        "fraud16.drop(columns=[\"device_hash\"], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebf6768a",
      "metadata": {},
      "source": [
        "Then I will convert the timestamp column to multiple columns. I will convert the timestamp column to hour, day of week, month and is_weekend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16d635e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert 'timestamp' to datetime\n",
        "fraud16['timestamp'] = pd.to_datetime(fraud16['timestamp'], format='ISO8601')\n",
        "\n",
        "# Add new columns\n",
        "fraud16['hour'] = fraud16['timestamp'].dt.hour\n",
        "fraud16['day_of_week'] = fraud16['timestamp'].dt.weekday\n",
        "fraud16['month'] = fraud16['timestamp'].dt.month\n",
        "fraud16['is_weekend'] = fraud16['timestamp'].dt.weekday.isin([5, 6])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c32ed2b6",
      "metadata": {},
      "source": [
        "I will use a `70/30 split` to train and test the model. I will use the `train_test_split` function from the `sklearn.model_selection` module to do this. I will also set the random state to 42 to ensure that the results are reproducible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "621f7c53",
      "metadata": {},
      "outputs": [],
      "source": [
        "fraud16_train, fraud16_test = msn.train_test_split(fraud16, test_size = 0.3, random_state = 42, stratify = fraud16[\"is_fraud\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0bd82e6",
      "metadata": {},
      "source": [
        "As before, I am going to train the model using the DecisionTreeClassifier from the sklearn.tree module. I am setting max depth to 5 as this is a good starting point. I will also set the random state to 42 to ensure that the results are reproducible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6e966d5",
      "metadata": {},
      "outputs": [],
      "source": [
        "numerical_cols = [\"amount\", \"time_since_last_transaction\", \"spending_deviation_score\", \"velocity_score\", \"geo_anomaly_score\", \"hour\", \"day_of_week\", \"month\", \"is_weekend\"]\n",
        "dt16 = tree.DecisionTreeClassifier(max_depth=5)\n",
        "dt16.fit(fraud16_train[numerical_cols], fraud16_train[\"is_fraud\"]) # First the columns I am using to predict, then the target column."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b8bd99a",
      "metadata": {},
      "source": [
        "Using this custom function, I can visualize the decision tree:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9936e6e",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_tree_classification(dt16, numerical_cols, ['False', 'True'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c10ae45",
      "metadata": {},
      "source": [
        "Now I will use the trained decision tree to predict if the transaction was fraudulent. I will use the test set to evaluate the performance of the model. Also, I will use the train set to evaluate if the model is overfitting the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bd25db4",
      "metadata": {},
      "outputs": [],
      "source": [
        "predictionsOnTrainset16 = dt16.predict(fraud16_train[numerical_cols])\n",
        "predictionsOnTestset16 = dt16.predict(fraud16_test[numerical_cols])\n",
        "\n",
        "accuracyTrain = calculate_accuracy(predictionsOnTrainset16, fraud16_train.is_fraud)\n",
        "accuracyTest = calculate_accuracy(predictionsOnTestset16, fraud16_test.is_fraud)\n",
        "\n",
        "print(f\"Model Performance:\")\n",
        "print(f\"Accuracy on Training Set: {accuracyTrain:.2%}\")\n",
        "print(f\"Accuracy on Test Set: {accuracyTest:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e15e13d",
      "metadata": {},
      "source": [
        "The accuracy of the model is 96.4%. I read online that the 94% accuracy is probably because the dataset is heavy imbalanced. There are a lot more non-fraudulent transactions than fraudulent transactions. Either way, I still think it was a learning experience. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "071d8e47",
      "metadata": {},
      "source": [
        "# Assignment 17 - Regression\n",
        "\n",
        "In this assignment, I am going to train a decision tree to predict a the body mass (gram) of a penguin. For this assignment, I will use the penguins dataset. The dataset contains information about the species, island, bill length, bill depth, flipper length, and body mass of penguins."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9472d185",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import hashlib as hl\n",
        "import seaborn as sns\n",
        "import graphviz as gv\n",
        "import sklearn.tree as tree\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.model_selection as msn\n",
        "\n",
        "penguins17 = sns.load_dataset(\"penguins\")\n",
        "penguins17.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b8fb6e6",
      "metadata": {},
      "source": [
        "I will first use a correlation matrix to see if there are any correlations between the columns. I will use the `corr()` function from the pandas library to do this. I will also use the `seaborn` library to visualize the correlation matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "438b1b51",
      "metadata": {},
      "outputs": [],
      "source": [
        "penguins17.corr(numeric_only=True).style.background_gradient(cmap='coolwarm', axis=None).format(precision=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "948007b4",
      "metadata": {},
      "source": [
        "Then, I will spit the dataset into a train and test set. I will use the `train_test_split` function from the `sklearn.model_selection` module to do this. I will also set the random state to 42 to ensure that the results are reproducible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "960caef9",
      "metadata": {},
      "outputs": [],
      "source": [
        "penguins17_train, penguins17_test = msn.train_test_split(penguins17, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33200f9c",
      "metadata": {},
      "source": [
        "Next, I will train the model using the DecisionTreeRegressor from the sklearn.tree module. I am setting max depth to 4 as this is a good starting point. I will also set the random state to 42 to ensure that the results are reproducible. I am going to use all the columns except the species, sex and island columns because these columns are not supported by the DecisionTreeRegressor. Also I will not be using the body mass column because this is the column I am trying to predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55faec59",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop rows with NaN in the target column\n",
        "penguins17_train = penguins17_train.dropna(subset=['body_mass_g'])\n",
        "\n",
        "# Define features and target\n",
        "features17 = ['flipper_length_mm', 'bill_depth_mm', 'bill_length_mm']\n",
        "dt_regression17 = tree.DecisionTreeRegressor(max_depth=4)\n",
        "\n",
        "# Fit the model\n",
        "dt_regression17.fit(penguins17_train[features17].values, penguins17_train['body_mass_g'].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c139d4f",
      "metadata": {},
      "source": [
        "I will make a custom function to visualize the regression decision tree. This is done with the code below. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caae6dc0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn import tree\n",
        "import graphviz\n",
        "\n",
        "def plot_tree_regression(model, features):\n",
        "    dot_data = tree.export_graphviz(model, out_file=None, feature_names=features, filled=True, rounded=True, special_characters=True)\n",
        "    graph = graphviz.Source(dot_data)  \n",
        "    return graph "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be3d3637",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_tree_regression(dt_regression17, features17)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "008ff42a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_rmse(predictions, actuals):\n",
        "    if(len(predictions) != len(actuals)):\n",
        "        raise Exception(\"The amount of predictions did not equal the amount of actuals\")\n",
        "    \n",
        "    return (((predictions - actuals) ** 2).sum() / len(actuals)) ** (1/2)\n",
        "\n",
        "def calculate_rmsle(predictions, actuals):\n",
        "    if(len(predictions) != len(actuals)):\n",
        "        raise Exception(\"The amount of predictions did not equal the amount of actuals\")\n",
        "    \n",
        "    return (((np.log1p(predictions) - np.log1p(actuals)) ** 2).sum() / len(actuals)) ** (1/2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "713133fe",
      "metadata": {},
      "source": [
        "The code above is for calculating the RMSE. The RMSE is the root mean squared error. It is a measure of how well the model fits the data. The lower the RMSE, the better the model fits the data. The RMSE is calculated by taking the square root of the mean of the squared differences between the predicted and actual values.\n",
        "\n",
        "Below is the result of the RMSE calculation for the train and test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ede54165",
      "metadata": {},
      "outputs": [],
      "source": [
        "predictionsOnTrainset17 = dt_regression17.predict(penguins17_train[features17].values)\n",
        "predictionsOnTestset17 = dt_regression17.predict(penguins17_test[features17].values)\n",
        "\n",
        "rmseTrain17 = calculate_rmse(predictionsOnTrainset17, penguins17_train.body_mass_g)\n",
        "rmseTest17 = calculate_rmse(predictionsOnTestset17, penguins17_test.body_mass_g)\n",
        "\n",
        "print(\"RMSE on training set \" + str(rmseTrain17))\n",
        "print(\"RMSE on test set \" + str(rmseTest17))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d28c9cc",
      "metadata": {},
      "source": [
        "The RMSE on the training set is significantly lower than the RMSE on the test set. This suggests that the model may be overfitting the training data. Overfitting occurs when a model learns the training data too well, including its noise and outliers, which reduces its ability to generalize to unseen data. This is evident here as the model performs much better on the training set compared to the test set.\n",
        "\n",
        "To address this issue, I will experiment with different variations of the model by adjusting the `max_depth` parameter of the decision tree and the columns used for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67a655bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "def find_best_tree_model(\n",
        "    features,\n",
        "    train_df,\n",
        "    test_df,\n",
        "    target_column,\n",
        "    model_type='regressor',            # 'regressor' or 'classifier'\n",
        "    algorithm='decision_tree',         # 'decision_tree' or 'random_forest'\n",
        "    depth_range=(3, 8),                # inclusive range of depths to try\n",
        "    n_estimators_list=(100,),          # only used if algorithm='random_forest'\n",
        "):\n",
        "    \"\"\"\n",
        "    Searches feature subsets, tree depths, (and for RF: n_estimators)\n",
        "    to find the best DecisionTree/RandomForest model.\n",
        "\n",
        "    Parameters:\n",
        "        features (list of str): columns to consider as inputs\n",
        "        train_df, test_df (pd.DataFrame): train & test sets\n",
        "        target_column (str): name of the y‐column\n",
        "        model_type (str): 'regressor' or 'classifier'\n",
        "        algorithm (str): 'decision_tree' or 'random_forest'\n",
        "        depth_range (tuple): (min_depth, max_depth) inclusive\n",
        "        n_estimators_list (iterable of int): RF n_estimators to try\n",
        "\n",
        "    Returns:\n",
        "        dict:\n",
        "            'best_features': tuple of column names\n",
        "            'best_depth': int\n",
        "            'best_n_estimators': int or None\n",
        "            'train_score': float\n",
        "            'test_score': float\n",
        "            'best_model': fitted estimator\n",
        "    \"\"\"\n",
        "\n",
        "    # choose classes & metric functions\n",
        "    if model_type == 'regressor':\n",
        "        TreeCls = DecisionTreeRegressor\n",
        "        RFCls   = RandomForestRegressor\n",
        "        score_fn = lambda y_pred, y_true: np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "        better = lambda a, b: a < b      # lower RMSE is better\n",
        "        default_n = None\n",
        "    else:\n",
        "        TreeCls = DecisionTreeClassifier\n",
        "        RFCls   = RandomForestClassifier\n",
        "        score_fn = lambda y_pred, y_true: accuracy_score(y_true, y_pred)\n",
        "        better = lambda a, b: a > b      # higher accuracy is better\n",
        "        default_n = 1\n",
        "\n",
        "    best = {\n",
        "        'best_features': None,\n",
        "        'best_depth': None,\n",
        "        'best_n_estimators': default_n,\n",
        "        'train_score': -np.inf if model_type=='classifier' else np.inf,\n",
        "        'test_score': -np.inf if model_type=='classifier' else np.inf,\n",
        "        'best_model': None\n",
        "    }\n",
        "\n",
        "    # helper to instantiate\n",
        "    def make_model(max_depth, n_estimators):\n",
        "        if algorithm == 'decision_tree':\n",
        "            return TreeCls(max_depth=max_depth)\n",
        "        else:\n",
        "            return RFCls(max_depth=max_depth, n_estimators=n_estimators)\n",
        "\n",
        "    all_subsets = (\n",
        "        [ (f,) for f in features ] +\n",
        "        [ subset for r in range(2, len(features)+1)\n",
        "                 for subset in combinations(features, r) ]\n",
        "    )\n",
        "\n",
        "    for subset in all_subsets:\n",
        "        X_train = train_df[list(subset)].values\n",
        "        X_test  = test_df[list(subset)].values\n",
        "        y_train = train_df[target_column].values\n",
        "        y_test  = test_df[target_column].values\n",
        "\n",
        "        for depth in range(depth_range[0], depth_range[1] + 1):\n",
        "            for n_est in (n_estimators_list if algorithm=='random_forest' else [None]):\n",
        "                model = make_model(depth, n_est)\n",
        "                model.fit(X_train, y_train)\n",
        "\n",
        "                y_pred_train = model.predict(X_train)\n",
        "                y_pred_test  = model.predict(X_test)\n",
        "\n",
        "                train_score = score_fn(y_pred_train, y_train)\n",
        "                test_score  = score_fn(y_pred_test, y_test)\n",
        "\n",
        "                # print progress\n",
        "                print(f\"Features={list(subset)}, depth={depth}\"\n",
        "                      f\"{(', n_est=' + str(n_est)) if n_est is not None else ''}\"\n",
        "                      f\" → train={train_score:.4f}, test={test_score:.4f}\")\n",
        "\n",
        "                if better(test_score, best['test_score']):\n",
        "                    best.update({\n",
        "                        'best_features': subset,\n",
        "                        'best_depth': depth,\n",
        "                        'best_n_estimators': n_est,\n",
        "                        'train_score': train_score,\n",
        "                        'test_score': test_score,\n",
        "                        'best_model': model\n",
        "                    })\n",
        "\n",
        "    return best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f67fb55",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop rows with NaN values in the features or target column\n",
        "penguins17_train_clean = penguins17_train.dropna(subset=features17 + ['body_mass_g'])\n",
        "penguins17_test_clean = penguins17_test.dropna(subset=features17 + ['body_mass_g'])\n",
        "\n",
        "result = find_best_tree_model(\n",
        "    features=features17,\n",
        "    train_df=penguins17_train_clean,\n",
        "    test_df=penguins17_test_clean,\n",
        "    target_column='body_mass_g',\n",
        "    model_type='regressor',\n",
        "    algorithm='decision_tree',\n",
        "    depth_range=(3, 8)\n",
        ")\n",
        "\n",
        "print(f\"Best Features: {result['best_features']}\")\n",
        "print(f\"Best Model Depth: {result['best_depth']}\")\n",
        "print(f\"Train RMSE: {result['train_score']:.2f}\")\n",
        "print(f\"Test RMSE: {result['test_score']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e583cda",
      "metadata": {},
      "source": [
        "According to this very complex algorithm (written by me and my friend ChatGPT), the best RMSE is achieved with a max depth is 3 while using all the columns. The RMSE is 335.4 on the test set and 350.4 on the train set."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e2c5d03",
      "metadata": {},
      "source": [
        "# Assignment 18\n",
        "\n",
        "In this assignment, I am going to train a decision tree to predict a persons calorie expenditure based on their characteristics. I will use a new dataset for this assignment. I found a dataset on Kaggle that contains information about the characteristics of individuals and their calorie expenditure. The dataset can be found here: [https://www.kaggle.com/competitions/playground-series-s5e5/data](https://www.kaggle.com/competitions/playground-series-s5e5/data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0772b953",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import hashlib as hl\n",
        "import graphviz as gv\n",
        "import sklearn.tree as tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "expenditure18 = pd.read_csv(\"Datasets\\\\calorie_expenditure.csv\")\n",
        "expenditure18.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "887e3898",
      "metadata": {},
      "source": [
        "I will first use a correlation matrix to see if there are any correlations between the columns. I will use the `corr()` function from the pandas library to do this. I will also use the `seaborn` library to visualize the correlation matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c52330ca",
      "metadata": {},
      "outputs": [],
      "source": [
        "expenditure18.corr(numeric_only=True).style.background_gradient(cmap='coolwarm', axis=None).format(precision=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e24b7092",
      "metadata": {},
      "source": [
        "Then, I will spit the dataset into a train and test set. I will use the `train_test_split` function from the `sklearn.model_selection` module to do this. I will also set the random state to 42 to ensure that the results are reproducible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6ff062a",
      "metadata": {},
      "outputs": [],
      "source": [
        "expenditure18_train, expenditure18_test = msn.train_test_split(expenditure18, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d23b1dd",
      "metadata": {},
      "source": [
        "Next, I will train the model using the DecisionTreeRegressor from the sklearn.tree module. I am setting max depth to 4 as this is a good starting point. I will also set the random state to 42 to ensure that the results are reproducible. I am not going to use only numerical columns because categorical columns are not supported by the DecisionTreeRegressor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1e08374",
      "metadata": {},
      "outputs": [],
      "source": [
        "features18 = ['Duration', 'Heart_Rate', 'Body_Temp']\n",
        "dt_regression18 = tree.DecisionTreeRegressor(max_depth = 4)\n",
        "dt_regression18.fit(expenditure18_train[features18].values, expenditure18_train['Calories'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c44a7e47",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_tree_regression(dt_regression18, features18)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a99f1146",
      "metadata": {},
      "outputs": [],
      "source": [
        "predictionsOnTrainset18 = dt_regression18.predict(expenditure18_train[features18].values)\n",
        "predictionsOnTestset18 = dt_regression18.predict(expenditure18_test[features18].values)\n",
        "\n",
        "rmseTrain18 = calculate_rmse(predictionsOnTrainset18, expenditure18_train.Calories)\n",
        "rmseTest18 = calculate_rmse(predictionsOnTestset18, expenditure18_test.Calories)\n",
        "\n",
        "print(\"RMSE on training set \" + str(rmseTrain18))\n",
        "print(\"RMSE on test set \" + str(rmseTest18))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d7b378f",
      "metadata": {},
      "source": [
        "The RMSE is marginaly lower on the test set than on the train set. This suggests that the model is not overfitting the training data and is able to generalize well to unseen data. The model performs well on both the training and test sets, indicating that it has learned the underlying patterns in the data effectively. Still, I will experiment with different variations of the model by adjusting the `max_depth` parameter of the decision tree and the columns used for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3cc3f85",
      "metadata": {},
      "outputs": [],
      "source": [
        "result = find_best_tree_model(\n",
        "  features=features18,\n",
        "  train_df=expenditure18_train,\n",
        "  test_df=expenditure18_test,\n",
        "  target_column='Calories',\n",
        "  model_type='regressor',\n",
        "  algorithm='decision_tree',\n",
        "  depth_range=(3, 8)\n",
        ")\n",
        "\n",
        "print(f\"Best Features: {result['best_features']}\")\n",
        "print(f\"Best Model Depth: {result['best_depth']}\")\n",
        "print(f\"Train RMSE: {result['train_score']:.2f}\")\n",
        "print(f\"Test RMSE: {result['test_score']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f6e237a",
      "metadata": {},
      "source": [
        "After running my custom function for finding the best RMSE, I put my findings the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71f0a3a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "features18_2 = ['Duration', 'Heart_Rate']\n",
        "\n",
        "dt_regression18_2 = tree.DecisionTreeRegressor(max_depth = 7)\n",
        "dt_regression18_2.fit(expenditure18_train[features18_2].values, expenditure18_train['Calories'].values)\n",
        "\n",
        "predictionsOnTrainset18_2 = dt_regression18_2.predict(expenditure18_train[features18_2].values)\n",
        "predictionsOnTestset18_2 = dt_regression18_2.predict(expenditure18_test[features18_2].values)\n",
        "\n",
        "rmseTrain18_2 = calculate_rmse(predictionsOnTrainset18_2, expenditure18_train.Calories)\n",
        "rmseTest18_2 = calculate_rmse(predictionsOnTestset18_2, expenditure18_test.Calories)\n",
        "\n",
        "print(\"RMSE on training set \" + str(rmseTrain18_2))\n",
        "print(\"RMSE on test set \" + str(rmseTest18_2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ddb0a82",
      "metadata": {},
      "source": [
        "The result of testing the model with different combinations of columns and max depth is shown above. The best RMSE is achieved with a depth of 7 while using `Duration` and `Heart_Rate` columns. The RMSE is only 11.53 on the test set and 11.52 on the train set. The overall RMSE is very low, which is a good sign. But the test set RMSE is a bit higher than the train set RMSE. So there has been a increase of accuracy, but a decrease of generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84f40095",
      "metadata": {},
      "source": [
        "If I use the model to predict the calorie expenditure of myself, I get the following result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2e65dd3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "print(dt_regression18_2.predict(np.array([[60, 60]]))) # One hour at 60 BPM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5f8c34e",
      "metadata": {},
      "source": [
        "## Question\n",
        " \n",
        "> **Is the RMSE just a magical number or is it tied to the measurement of the data?**  \n",
        "> **For example, does an RMSE of 11 mean I am 11 calories off or is it just an abstract number unrelated to the data?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8d3836c",
      "metadata": {},
      "source": [
        "### 🤖 Answer from ChatGPT\n",
        "\n",
        "**RMSE (Root Mean Squared Error)** is **not a magical number**.  \n",
        "It is very much tied to the **scale** and **units** of your target variable — in this case, **calorie expenditure**.\n",
        "\n",
        "If your `Calories` column is measured in calories (which it almost certainly is), then:\n",
        "\n",
        "##### ✅ **An RMSE of 11.53 means:**\n",
        "> ➡️ **On average, your model's predictions are off by about 11.53 calories.**\n",
        "\n",
        "#### 🔍 A Bit More Depth\n",
        "\n",
        "- 📐 **RMSE** is calculated as the **square root of the average of the squared differences** between predicted and actual values.\n",
        "- ⚖️ It **penalizes larger errors more heavily** than smaller ones because of the squaring.\n",
        "- 📎 The **unit of RMSE** is the same as your target variable (in this case, calories).\n",
        "\n",
        "#### 📊 Real Example\n",
        "\n",
        "If someone actually burned **250 calories**, and your model predicted **238.5**,  \n",
        "that’s an error of **11.5 calories** — which would directly factor into the RMSE you're seeing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dacf4541",
      "metadata": {},
      "source": [
        "# Assignment 19 - Random Forest\n",
        "\n",
        "In this assignment, I am going to train a random forest to predict Experience_Level based on the characteristics of a gym member. I will use the same dataset as in assignment 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4606908f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import hashlib as hl\n",
        "import graphviz as gv\n",
        "import sklearn.tree as tree\n",
        "import sklearn.ensemble as ens\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.model_selection as msn\n",
        "\n",
        "gym19 = pd.read_csv(\"Datasets\\\\gym_members_exercise_tracking.csv\")\n",
        "gym19.head()\n",
        "gym19.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cbd14aa",
      "metadata": {},
      "source": [
        "I will first check the correlation matrix to see if there are any correlations between the columns. I will use the `corr()` function from the pandas library to do this. I will also use the `seaborn` library to visualize the correlation matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c88424c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "gym19.corr(numeric_only=True).style.background_gradient(cmap='coolwarm', axis=None).format(precision=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afb842ff",
      "metadata": {},
      "source": [
        "First, I will define the features and target variable. The features are the columns that I will use to train the model and the target variable is the column that I want to predict. I have chosen the following columns based on the correlation matrix as features: 'Workout_Frequency (days/week)', 'Session_Duration (hours)', 'Calories_Burned', 'Water_Intake (liters)' and 'Fat_Percentage'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8eab144c",
      "metadata": {},
      "outputs": [],
      "source": [
        "features19 = ['Workout_Frequency (days/week)', 'Session_Duration (hours)', 'Calories_Burned']\n",
        "target19 = 'Experience_Level'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a53b7b2",
      "metadata": {},
      "source": [
        "Then, I will spit the dataset into a train and test set. I will use the `train_test_split` function from the `sklearn.model_selection` module to do this. I will also set the random state to 42 to ensure that the results are reproducible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82f85f4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "X19 = gym19[features19]\n",
        "y19 = gym19[target19]\n",
        "\n",
        "X_train19, X_test19, y_train19, y_test19 = msn.train_test_split(X19, y19, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0883c333",
      "metadata": {},
      "source": [
        "Next, I will train the model using RandomForestClassifier from the sklearn.ensemble module. I am setting the number of estimators to 100 as this is a good starting point. I will also set the random state to 42 to ensure that the results are reproducible. Also the `max_depth` is set to 6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce47e799",
      "metadata": {},
      "outputs": [],
      "source": [
        "clf19 = ens.RandomForestClassifier(n_estimators=100, max_depth=6, random_state=42)\n",
        "clf19.fit(X_train19, y_train19)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "858581d2",
      "metadata": {},
      "source": [
        "After training the model, I will use the trained random forest to predict the experience levels. I will use the test set to evaluate the performance of the model. Also, I will use the train set to evaluate if the model is overfitting the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "187767e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "predictionsOnTrainset19 = clf19.predict(X_train19)\n",
        "predictionsOnTestset19 = clf19.predict(X_test19)\n",
        "\n",
        "# 7. Calculate and print accuracy\n",
        "accuracyTrain19 = calculate_accuracy(y_train19, predictionsOnTrainset19)\n",
        "accuracyTest19 = calculate_accuracy(y_test19, predictionsOnTestset19)\n",
        "\n",
        "print(f\"Train Accuracy: {accuracyTrain19:0.4f}\")\n",
        "print(f\"Test Accuracy: {accuracyTest19:0.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5131375",
      "metadata": {},
      "source": [
        "The model achieved an accuracy of 91.10% on the test set and 90.60% on the training set. These results indicate that the model demonstrates strong generalization capabilities, as the performance on unseen data is consistent with the training data. The minimal difference between training and test accuracy suggests that the model is not overfitting and effectively captures the underlying patterns in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fdf6e64",
      "metadata": {},
      "source": [
        "Initially, the features selected for the model were 'Workout_Frequency (days/week)', 'Session_Duration (hours)', 'Calories_Burned', 'Water_Intake (liters)', and 'Fat_Percentage'. This configuration yielded an accuracy of 90.4% on the test set and 90% on the training set.\n",
        "\n",
        "Subsequently, the feature set was refined to include only 'Workout_Frequency (days/week)', 'Session_Duration (hours)', 'Calories_Burned', and 'Fat_Percentage'. This adjustment resulted in a marginally improved training set accuracy of 91.19%, but the test set accuracy slightly decreased to 90.41%. The higher training accuracy suggested potential overfitting, prompting further experimentation with feature selection.\n",
        "\n",
        "In the final iteration, the 'Fat_Percentage' and 'Water_Intake (liters)' features were excluded due to their lower correlation with the target variable. The model was retrained using the features 'Workout_Frequency (days/week)', 'Session_Duration (hours)', and 'Calories_Burned'. This approach, as detailed in the assignment above, aimed to balance model performance and generalization by focusing on the most relevant predictors. The final model achieved a training accuracy of 90.60% and a test accuracy of 91.10%, demonstrating strong generalization capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9d98be1",
      "metadata": {},
      "source": [
        "# Assignment 20 - Perceptron\n",
        "\n",
        "In this assignment, I will train a simple neural network (a multi-layer perceptron) to recognize handwritten digits from the MNIST dataset. I will use the fetch_openml function to load the dataset and the MLPClassifier from sklearn.neural_network for training. Hyperparameters such as learning rate and number of iterations will be adjusted to explore performance. At the end, I’ll evaluate how well the model performs on both the training and testing datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "explanation-imports",
      "metadata": {},
      "source": [
        "First, I will import all the necessary Python libraries. These include tools for visualization (seaborn, matplotlib), loading the dataset (fetch_openml), preprocessing (StandardScaler), training the model (MLPClassifier), and evaluating it (accuracy_score)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fe8cbea",
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "explanation-load-data",
      "metadata": {},
      "source": [
        "Now I am going to load the MNIST dataset, which contains 70,000 images of handwritten digits (0–9). Each image is 28x28 pixels, flattened into a 784-length vector. The labels are cast to integers for classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8342e137",
      "metadata": {},
      "outputs": [],
      "source": [
        "mnist20 = fetch_openml('mnist_784')\n",
        "X20 = mnist20.data\n",
        "y20 = mnist20.target.astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "explanation-scale-data",
      "metadata": {},
      "source": [
        "Before training the neural network, I need to scale the input data using StandardScaler. This standardizes the features to have zero mean and unit variance. Scaling is essential for gradient-based optimization to converge properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "137f6720",
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler20 = StandardScaler()\n",
        "X20_scaled = scaler20.fit_transform(X20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "explanation-split-data",
      "metadata": {},
      "source": [
        "Now I split the dataset into 70% training and 30% testing using train_test_split. This allows us to train the model on one portion and evaluate it on unseen data to measure generalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8133b74",
      "metadata": {},
      "outputs": [],
      "source": [
        "X20_train, X20_test, y20_train, y20_test = train_test_split(X20_scaled, y20, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "explanation-train-model",
      "metadata": {},
      "source": [
        "Here, I create and train a MLPClassifier model. I set the number of hidden units to 10 (a very small network), use a learning rate of 0.01, and limit training to 100 iterations. These parameters were chosen to keep training fast and interpretable for this experiment.\n",
        "\n",
        "After training, I use the model to make predictions on both the training and test sets, then evaluate the accuracy of those predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87b48e89",
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate20 = 0.01\n",
        "max_iter20 = 2000\n",
        "hidden_layer_sizes20 = 200\n",
        "\n",
        "model20 = MLPClassifier(\n",
        "  hidden_layer_sizes=hidden_layer_sizes20,\n",
        "  learning_rate_init=learning_rate20,\n",
        "  max_iter=max_iter20,\n",
        "  random_state=42\n",
        ")\n",
        "\n",
        "model20.fit(X20_train, y20_train)\n",
        "\n",
        "y20_train_pred = model20.predict(X20_train)\n",
        "y20_test_pred = model20.predict(X20_test)\n",
        "\n",
        "train_accuracy20 = accuracy_score(y20_train, y20_train_pred)\n",
        "test_accuracy20 = accuracy_score(y20_test, y20_test_pred)\n",
        "\n",
        "print(f'Training Accuracy: {train_accuracy20:.4f}')\n",
        "print(f'Test Accuracy: {test_accuracy20:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9020f453",
      "metadata": {},
      "source": [
        "The first try, I had an accuracy of 96% on the training set and 92% on the test set. This is a good start, but I want to improve the model's performance by tuning parameters and increasing the complexity of the network. The max iterations is set to 100, which is a good starting point. The learning rate is set to 0.01, which is a good starting point. The hidden layer size is set to 10, which is a good starting point.\n",
        "\n",
        "The second try, I had an accuracy of 99.6% on the training set and 96.7% on the test set. This is a good improvement. I used the following hyperparameters for this attempt: **Learning Rate**: 0.01, **Max Iterations**: 1000, **Hidden Layer Size**: 100. I will now try to increase the max iterations to 2000 and see if that improves the accuracy. I will also try to increase the hidden layer size to 200 and see if that improves the accuracy.\n",
        "\n",
        "The third try, I had an accuracy of 99.74% on the training set and 96.94% on the test set. This is my final result. I am happy with this result."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33490cfd",
      "metadata": {},
      "source": [
        "# Assignment 21\n",
        "\n",
        "In this assignment, I will train a simple neural network (a multi-layer perceptron) to predict the experience level of a gym member based on their characteristics. I will use the same dataset as in assignment 4."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c5577f6",
      "metadata": {},
      "source": [
        "First, I will import all the necessary Python libraries. These include tools for visualization (seaborn, matplotlib), loading the dataset (fetch_openml), preprocessing (StandardScaler), training the model (MLPClassifier), and evaluating it (accuracy_score)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e365b20c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "226c021d",
      "metadata": {},
      "source": [
        "Now I am going to load the gym members dataset which I chose in portfolio assignment 4. The dataset contains information about gym members, their exercise habits, and their demographics. I am going to predict the experience level of the gym members."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1653c722",
      "metadata": {},
      "outputs": [],
      "source": [
        "gym21 = pd.read_csv(\"Datasets\\\\gym_members_exercise_tracking.csv\")\n",
        "X21 = gym21[['Workout_Frequency (days/week)', 'Session_Duration (hours)', 'Calories_Burned']]\n",
        "y21 = gym21['Experience_Level']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b924fcbc",
      "metadata": {},
      "source": [
        "Before training the neural network, I need to scale the input data using StandardScaler. This standardizes the features to have zero mean and unit variance. Scaling is essential for gradient-based optimization to converge properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b74fa331",
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler21 = StandardScaler()\n",
        "X21_scaled = scaler21.fit_transform(X21)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7970d650",
      "metadata": {},
      "source": [
        "Now I split the dataset into 70% training and 30% testing using train_test_split. This allows us to train the model on one portion and evaluate it on unseen data to measure generalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb1239d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "X21_train, X21_test, y21_train, y21_test = train_test_split(X21_scaled, y21, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0330eb64",
      "metadata": {},
      "source": [
        "Here, I create and train a MLPClassifier model. I set the number of hidden units to 10 (a very small network), use a learning rate of 0.01, and limit training to 100 iterations. These parameters were chosen to keep training fast and interpretable for this experiment.\n",
        "\n",
        "After training, I use the model to make predictions on both the training and test sets, then evaluate the accuracy of those predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "741f445b",
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate21 = 0.05\n",
        "max_iter21 = 1010\n",
        "hidden_layer_sizes21 = 10\n",
        "\n",
        "model21 = MLPClassifier(\n",
        " hidden_layer_sizes=hidden_layer_sizes21,\n",
        " learning_rate_init=learning_rate21,\n",
        " max_iter=max_iter21,\n",
        " random_state=42\n",
        ")\n",
        "\n",
        "model21.fit(X21_train, y21_train)\n",
        "\n",
        "y21_train_pred = model21.predict(X21_train)\n",
        "y21_test_pred = model21.predict(X21_test)\n",
        "\n",
        "train_accuracy21 = accuracy_score(y21_train, y21_train_pred)\n",
        "test_accuracy21 = accuracy_score(y21_test, y21_test_pred)\n",
        "\n",
        "print(f'Training Accuracy: {train_accuracy21:.4f}')\n",
        "print(f'Test Accuracy: {test_accuracy21:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "513858de",
      "metadata": {},
      "source": [
        "The first try, I had an accuracy of 89.57% on the training set and 90.07% on the test set. This is a good start, but I want to improve the model's performance by tuning parameters and increasing the complexity of the network.\n",
        "\n",
        "The second try, I had an accuracy of 89.57% on the training set and 91.10% on the test set. I used the following hyperparameters for this attempt: **Learning Rate**: 0.05, **Max Iterations**: 1010, **Hidden Layer Size**: 10. This is a slight improvement in test accuracy compared to the first try, but the training accuracy remained the same. This is my final result as I am happy with this."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11a16dd5",
      "metadata": {},
      "source": [
        "The dataset and the columns I used for the training are the same as in assignment 19. The achieved accuracy is 91.10% is also the same as in assignment 19 (random forest). This is a good result. I am happy with this result. The model is able to generalize well to unseen data. The minimal difference between training and test accuracy suggests that the model is not overfitting and effectively captures the underlying patterns in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62ce2fe7",
      "metadata": {},
      "source": [
        "# Assignment 22\n",
        "\n",
        "In this assignment, I explore the application of the YOLO v8 model for object detection, specifically targeting traffic sign recognition. YOLO (You Only Look Once) is a state-of-the-art real-time object detection system known for its balance between speed and accuracy.\n",
        "\n",
        "To begin, I import a variety of libraries necessary for different tasks involved in this project. This includes libraries for image processing (`cv2`, `PIL`), data manipulation (`numpy`, `pandas`), visualization (`matplotlib`), and the YOLO framework itself (`ultralytics`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eca1958d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pathlib as plb\n",
        "import warnings as warn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from ultralytics import YOLO\n",
        "from IPython.display import display, Video\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Check YOLO installation\n",
        "import ultralytics\n",
        "ultralytics.checks()\n",
        "\n",
        "# Suppress specific warnings\n",
        "warn.filterwarnings(\"ignore\", category=ConvergenceWarning)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c05eec3",
      "metadata": {},
      "source": [
        "Loading the YOLO v8 model initializes the pretrained weights and network architecture. This step is crucial because it allows us to leverage transfer learning: the model has already learned to recognize a wide range of objects, and we will fine-tune it specifically for traffic sign detection.\n",
        "\n",
        "By using a pretrained model (`yolov8n.pt`), training time is reduced significantly, and the model can achieve better performance on smaller datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2aeefd2",
      "metadata": {},
      "outputs": [],
      "source": [
        "## I installed NVIDIA GPU Acceleration\n",
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"GPU device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2dde8c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Here I set the base path to the dataset containing traffic sign images.\n",
        "TSRpath = os.getcwd() +'/Datasets/TrafficSignRecognition/car'\n",
        "\n",
        "# Load the model to ensure the latest YOLO instance is used\n",
        "model = YOLO('yolov8m.pt')\n",
        "\n",
        "# The training process fine-tunes the model on our specific traffic sign dataset.\n",
        "# Train the YOLO model on the custom traffic sign dataset\n",
        "results = model.train(\n",
        "    data=os.path.join(TSRpath, 'data.yaml'),  # Path to dataset config\n",
        "    epochs=50,                                # Number of training epochs\n",
        "    imgsz=640,                                # Image size for training\n",
        "    batch=16,                                 # Batch size\n",
        "    name='tsd-r1',                            # Run name for experiment tracking\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu',  # Use GPU if available\n",
        "    workers=4,                                # Number of dataloader workers (adjust as needed)\n",
        "    patience=10,                              # Early stopping if no improvement\n",
        "    verbose=True                              # Print training progress\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "validation_explanation",
      "metadata": {},
      "source": [
        "After training, it is essential to evaluate the model on a validation dataset that it hasn't seen before. This step helps assess how well the model generalizes beyond the training data and identifies potential overfitting or underfitting.\n",
        "\n",
        "Metrics such as precision, recall, and mAP (mean Average Precision) give insights into the model's detection accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6fe86fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the trained model on the validation set to measure performance\n",
        "metrics = model.val(\n",
        "    data= TSRpath + '/data.yaml',\n",
        "    device='cuda'\n",
        ")\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "testing_explanation",
      "metadata": {},
      "source": [
        "Finally, the trained model is tested on a separate test set. This represents a real-world scenario where the model encounters new images and must detect traffic signs accurately. By setting a confidence threshold (`conf=0.25`), we control the sensitivity of detections, filtering out less certain predictions to reduce false positives. \n",
        "Saving the results allows for later review and analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bcba424",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the best weights from training to run inference on test images\n",
        "trained = YOLO('runs/detect/tsd-r1/weights/best.pt')\n",
        "\n",
        "# Run prediction on test images folder and save outputs\n",
        "results = trained.predict(source=TSRpath + '/test/images', conf=0.25, save=True)\n",
        "print(\"Inference completed, results saved in runs/detect/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "visualization_explanation",
      "metadata": {},
      "source": [
        "Visualization is a powerful tool to assess how well the model detects traffic signs in images. By displaying sample prediction images, we can judge the quality of detections, bounding box accuracy, and possible areas for improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f9b548b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display a sample detection image from the inference results\n",
        "from IPython.display import display, Image\n",
        "sample = 'runs/detect/tsd-r12'\n",
        "if os.path.exists(sample):\n",
        "    # Display the first available predicted image if the sample does not exist\n",
        "    import glob\n",
        "    predicted_images = glob.glob('runs/detect/tsd-r12/*.jpg')\n",
        "    if predicted_images:\n",
        "        display(Image(filename=predicted_images[1]))\n",
        "    else:\n",
        "        print(\"No predicted images found in runs/detect/predict/\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
