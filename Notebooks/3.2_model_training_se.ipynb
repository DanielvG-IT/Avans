{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "332653ab",
   "metadata": {},
   "source": [
    "# Model Training Sentence Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "121055a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/External/Development/Projects/School/L2S1LU2-Recommendation/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from helpers.functs.StudentProfile import StudentProfile\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import ast\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('../Data/Cleaned/cleaned_dataset_hard-NLP.csv')\n",
    "\n",
    "# Loading uncleaned dataset for feedback names, etc. that have not seen NLP for user friendliness\n",
    "raw_df = pd.read_csv('../Data/Raw/Uitgebreide_VKM_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30988a73",
   "metadata": {},
   "source": [
    "Some of the first steps of preparing the data will be the same as we did in the training of the BOW model. This is why we won't explain all those steps again in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c41ed59",
   "metadata": {},
   "source": [
    "## 0. Mocking a student profile (Copy of 3.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "103119a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "student = StudentProfile(\n",
    "    current_study= \"Kunst & Onderzoek\",\n",
    "    interests=[\n",
    "        \"Tekening\",\n",
    "        \"Animatie\",\n",
    "        \"Kunst\",\n",
    "        \"Artistiek\",\n",
    "        \"Het vermaken van mensen. Via zingen, dansen, toneel. Graag op het podium. \"\n",
    "    ],\n",
    "    wanted_study_credit_range=(15, 30),\n",
    "    location_preference=[\"Den Bosch\", \"Breda\", \"Tilburg\"],\n",
    "    learning_goals=[\"Carrière groei\", \"Sociale vaardigheden\", \"Zelfverzekerheid\", \"Vermaken\"],\n",
    "    level_preference=[\"NLQF5\", \"NLQF6\"],\n",
    "    preferred_language=\"NL\",\n",
    ")\n",
    "\n",
    "matching_models = [388, 392, 191, 385, 386, 379, 389, 377, 233]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8bbdf4",
   "metadata": {},
   "source": [
    "Creating a filtered dataset. Copy of the dataset used for comparison later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ebe3736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of modules: 211\n",
      "Number of modules after filtering: 211\n"
     ]
    }
   ],
   "source": [
    "# Create filtered module and save. The filtered one won't be used by TF-IDF because that would create bias. (Smaller amount of modules compared > easier higher scores)\n",
    "filtered_df = df.copy()\n",
    "\n",
    "# Helper to normalize the list-like location strings such as \"['Den Bosch', 'Tilburg']\"\n",
    "def normalize_locations(series):\n",
    "    def _to_list(val):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(str(val))\n",
    "            if isinstance(parsed, list):\n",
    "                return [str(x).strip().lower() for x in parsed]\n",
    "            return [str(parsed).strip().lower()]\n",
    "        except Exception:\n",
    "            return [str(val).strip().lower()]\n",
    "    return series.apply(_to_list)\n",
    "\n",
    "# --- 1. Study credits range ---\n",
    "if hasattr(student, \"wanted_study_credit_range\") and student.wanted_study_credit_range is not None:\n",
    "    min_cred, max_cred = student.wanted_study_credit_range\n",
    "    filtered_df = filtered_df[(filtered_df[\"studycredit\"] >= min_cred) & (filtered_df[\"studycredit\"] <= max_cred)]\n",
    "\n",
    "# --- 2. Location preference ---\n",
    "if hasattr(student, \"location_preference\") and student.location_preference:\n",
    "    all_locs_filtered = normalize_locations(filtered_df[\"location\"])\n",
    "    loc_prefs_norm = [str(x).strip().lower() for x in student.location_preference]\n",
    "    loc_mask = all_locs_filtered.apply(lambda lst: any(x in loc_prefs_norm for x in lst))\n",
    "    filtered_df = filtered_df[loc_mask]\n",
    "\n",
    "# --- 3. Language of the module vs preferred language of the student ---\n",
    "# Pretty complicated to include and won't be of any use anyways since tf-idf won't be able to link interests written in difference language than de modules\n",
    "\n",
    "# --- 4. Level preference (e.g. NLQF levels) ---\n",
    "if hasattr(student, \"level_preference\") and student.level_preference:\n",
    "    level_prefs = [str(x).strip().lower() for x in student.level_preference]\n",
    "    filtered_df = filtered_df[filtered_df[\"level\"].astype(str).str.lower().isin(level_prefs)]\n",
    "\n",
    "# --- 5. Availability > 0 ---\n",
    "filtered_df = filtered_df[filtered_df[\"available_spots\"] > 0]\n",
    "\n",
    "print(f\"Original number of modules: {len(df)}\")\n",
    "print(f\"Number of modules after filtering: {len(filtered_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac8be4a",
   "metadata": {},
   "source": [
    "# 1. Combining Relevant Text Columns of Modules Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d7a7367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>159</td>\n",
       "      <td>kennismak psychologi modul ler gedrag jezelf a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160</td>\n",
       "      <td>learn work abroad student kiez binn stam oplei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>161</td>\n",
       "      <td>proactiev zorgplann jeroen bosch ziekenhuis gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>162</td>\n",
       "      <td>rouw verlies modul stil gestan rouw verlies va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>163</td>\n",
       "      <td>acuut complex zorg modul student verdiep acut ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                               text\n",
       "0  159  kennismak psychologi modul ler gedrag jezelf a...\n",
       "1  160  learn work abroad student kiez binn stam oplei...\n",
       "2  161  proactiev zorgplann jeroen bosch ziekenhuis gr...\n",
       "3  162  rouw verlies modul stil gestan rouw verlies va...\n",
       "4  163  acuut complex zorg modul student verdiep acut ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine relevant text columns \n",
    "big_string = (\n",
    "    df[\"name\"].fillna(\"\") + \" \" +\n",
    "    df[\"description\"].fillna(\"\") + \" \" +\n",
    "    df[\"learningoutcomes\"].fillna(\"\") + \" \" +\n",
    "    df[\"module_tags\"].apply(lambda x: \" \".join(x) if isinstance(x, list) else \"\")\n",
    ")\n",
    "\n",
    "big_df = pd.DataFrame({\n",
    "    \"id\": df[\"id\"],\n",
    "    \"text\": big_string\n",
    "})\n",
    "\n",
    "big_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a8154b",
   "metadata": {},
   "source": [
    "## 2. Vectorizing dataset\n",
    "This time we'll be using sentence embedding for our vectorization. We selected SBERT with the multilingual model paraphrase-multilingual-MiniLM-L12-v2 to handle both Dutch and English inputs and content. This model provides strong semantic understanding, allowing it to capture the meaning of student profiles and course descriptions very well. It also embeds both languages into the same vector space, enabling accurate cross-language comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf81fcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading sentence model\n",
    "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dc75112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 7/7 [00:27<00:00,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0254322  -0.09378675 -0.01488835 ...  0.06911291  0.01060686\n",
      "  -0.01961194]\n",
      " [-0.12032837 -0.13125998 -0.00902268 ...  0.01264864  0.16957617\n",
      "  -0.01915479]\n",
      " [-0.0643099  -0.10150067 -0.01213171 ...  0.00204279  0.05127817\n",
      "   0.04092983]\n",
      " ...\n",
      " [-0.06131046 -0.09119403 -0.01486532 ...  0.03288979  0.10722221\n",
      "  -0.04470365]\n",
      " [-0.04897249 -0.06143222 -0.0092546  ... -0.0023255   0.006597\n",
      "  -0.02160808]\n",
      " [-0.02588925  0.10865548 -0.01675073 ...  0.09578825  0.04484264\n",
      "  -0.02644189]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Encode big_df text with sentence embeddings\n",
    "big_df_embeddings = model.encode(big_df[\"text\"].tolist(), show_progress_bar=True)\n",
    "big_df_embeddings = np.array(big_df_embeddings)\n",
    "print(big_df_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
