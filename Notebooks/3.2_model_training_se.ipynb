{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "332653ab",
   "metadata": {},
   "source": [
    "# Model Training Sentence Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "121055a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.functs.StudentProfile import StudentProfile\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('../Data/Cleaned/cleaned_dataset_hard-NLP.csv')\n",
    "\n",
    "# Loading uncleaned dataset for feedback names, etc. that have not seen NLP for user friendliness\n",
    "raw_df = pd.read_csv('../Data/Raw/Uitgebreide_VKM_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30988a73",
   "metadata": {},
   "source": [
    "Some of the first steps of preparing the data will be the same as we did in the training of the BOW model. This is why we won't explain all those steps again in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c41ed59",
   "metadata": {},
   "source": [
    "## 0. Mocking a student profile (Copy of 3.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "103119a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "student = StudentProfile(\n",
    "    current_study= \"Kunst & Onderzoek\",\n",
    "    interests=[\n",
    "        \"Tekening\",\n",
    "        \"Animatie\",\n",
    "        \"Kunst\",\n",
    "        \"Artistiek\",\n",
    "        \"Het vermaken van mensen. Via zingen, dansen, toneel. Graag op het podium. \"\n",
    "    ],\n",
    "    wanted_study_credit_range=(15, 30),\n",
    "    location_preference=[\"Den Bosch\", \"Breda\", \"Tilburg\"],\n",
    "    learning_goals=[\"CarriÃ¨re groei\", \"Sociale vaardigheden\", \"Zelfverzekerheid\", \"Vermaken\"],\n",
    "    level_preference=[\"NLQF5\", \"NLQF6\"],\n",
    "    preferred_language=\"NL\",\n",
    ")\n",
    "\n",
    "matching_models = [388, 392, 191, 385, 386, 379, 389, 377, 233]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8bbdf4",
   "metadata": {},
   "source": [
    "Creating a filtered dataset. Copy of the dataset used for comparison later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1ebe3736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of modules: 211\n",
      "Number of modules after filtering: 211\n"
     ]
    }
   ],
   "source": [
    "# Create filtered module and save. The filtered one won't be used by TF-IDF because that would create bias. (Smaller amount of modules compared > easier higher scores)\n",
    "filtered_df = df.copy()\n",
    "\n",
    "# Helper to normalize the list-like location strings such as \"['Den Bosch', 'Tilburg']\"\n",
    "def normalize_locations(series):\n",
    "    def _to_list(val):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(str(val))\n",
    "            if isinstance(parsed, list):\n",
    "                return [str(x).strip().lower() for x in parsed]\n",
    "            return [str(parsed).strip().lower()]\n",
    "        except Exception:\n",
    "            return [str(val).strip().lower()]\n",
    "    return series.apply(_to_list)\n",
    "\n",
    "# --- 1. Study credits range ---\n",
    "if hasattr(student, \"wanted_study_credit_range\") and student.wanted_study_credit_range is not None:\n",
    "    min_cred, max_cred = student.wanted_study_credit_range\n",
    "    filtered_df = filtered_df[(filtered_df[\"studycredit\"] >= min_cred) & (filtered_df[\"studycredit\"] <= max_cred)]\n",
    "\n",
    "# --- 2. Location preference ---\n",
    "if hasattr(student, \"location_preference\") and student.location_preference:\n",
    "    all_locs_filtered = normalize_locations(filtered_df[\"location\"])\n",
    "    loc_prefs_norm = [str(x).strip().lower() for x in student.location_preference]\n",
    "    loc_mask = all_locs_filtered.apply(lambda lst: any(x in loc_prefs_norm for x in lst))\n",
    "    filtered_df = filtered_df[loc_mask]\n",
    "\n",
    "# --- 3. Language of the module vs preferred language of the student ---\n",
    "# Pretty complicated to include and won't be of any use anyways since tf-idf won't be able to link interests written in difference language than de modules\n",
    "\n",
    "# --- 4. Level preference (e.g. NLQF levels) ---\n",
    "if hasattr(student, \"level_preference\") and student.level_preference:\n",
    "    level_prefs = [str(x).strip().lower() for x in student.level_preference]\n",
    "    filtered_df = filtered_df[filtered_df[\"level\"].astype(str).str.lower().isin(level_prefs)]\n",
    "\n",
    "# --- 5. Availability > 0 ---\n",
    "filtered_df = filtered_df[filtered_df[\"available_spots\"] > 0]\n",
    "\n",
    "print(f\"Original number of modules: {len(df)}\")\n",
    "print(f\"Number of modules after filtering: {len(filtered_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac8be4a",
   "metadata": {},
   "source": [
    "# 1. Combining Relevant Text Columns of Modules Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5d7a7367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine relevant text columns \n",
    "# big_string = (\n",
    "#     df[\"name\"].fillna(\"\") + \" \" +\n",
    "#     df[\"description\"].fillna(\"\") + \" \" +\n",
    "#     df[\"learningoutcomes\"].fillna(\"\") + \" \" +\n",
    "#     df[\"module_tags\"].apply(lambda x: \" \".join(x) if isinstance(x, list) else \"\")\n",
    "# )\n",
    "\n",
    "# stringified_df = pd.DataFrame({\n",
    "#     \"id\": df[\"id\"],\n",
    "#     \"text\": big_string\n",
    "# })\n",
    "\n",
    "# stringified_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a8154b",
   "metadata": {},
   "source": [
    "## 2. Vectorizing dataset\n",
    "This time we'll be using sentence embedding for our vectorization. We selected SBERT with the multilingual model paraphrase-multilingual-MiniLM-L12-v2 to handle both Dutch and English inputs and content. This model provides strong semantic understanding, allowing it to capture the meaning of student profiles and course descriptions very well. It also embeds both languages into the same vector space, enabling accurate cross-language comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bf81fcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading sentence model\n",
    "# model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9dc75112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encode big_df text with sentence embeddings\n",
    "# big_df_embeddings = model.encode(stringified_df[\"text\"].tolist(), show_progress_bar=True)\n",
    "# big_df_embeddings = np.array(big_df_embeddings)\n",
    "# big_df_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "56f09899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_embedded_dataframe = pd.DataFrame({\n",
    "#     \"id\": stringified_df[\"id\"],\n",
    "#     \"sentence_embedding_vector\": list(big_df_embeddings)  \n",
    "# })\n",
    "# sentence_embedded_dataframe.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988b6ed6",
   "metadata": {},
   "source": [
    "## 3. Exporting Dataset\n",
    "For tf-idf we just ran the whole notebook everytime since it only takes 0.8s. However now more computations are needed and it takes around 5 seconds so we decided to export it so it doesn't have to be calculated everytime. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2e755d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_csv = '../Data/Vectorized/sentence_embedded_dataframe.csv'\n",
    "# sentence_embedded_dataframe.to_csv(output_csv, index=False)\n",
    "# print('Saved dataframe to', output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1d4a79",
   "metadata": {},
   "source": [
    "## 3.1 Importing the Exported Dataset\n",
    "After performing sentence embedding the final shape was (211, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f8c060ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence_embedding_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>159</td>\n",
       "      <td>[-2.54321918e-02 -9.37866718e-02 -1.48883555e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160</td>\n",
       "      <td>[-1.20328344e-01 -1.31259754e-01 -9.02268570e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>161</td>\n",
       "      <td>[-6.43099099e-02 -1.01500481e-01 -1.21317105e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>162</td>\n",
       "      <td>[ 4.75589335e-02 -1.63877644e-02 -1.77945886e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>163</td>\n",
       "      <td>[-6.94717616e-02 -2.24891558e-01 -1.35854846e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                          sentence_embedding_vector\n",
       "0  159  [-2.54321918e-02 -9.37866718e-02 -1.48883555e-...\n",
       "1  160  [-1.20328344e-01 -1.31259754e-01 -9.02268570e-...\n",
       "2  161  [-6.43099099e-02 -1.01500481e-01 -1.21317105e-...\n",
       "3  162  [ 4.75589335e-02 -1.63877644e-02 -1.77945886e-...\n",
       "4  163  [-6.94717616e-02 -2.24891558e-01 -1.35854846e-..."
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_modules = pd.read_csv('../Data/Vectorized/sentence_embedded_dataframe.csv')\n",
    "embedded_modules.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
