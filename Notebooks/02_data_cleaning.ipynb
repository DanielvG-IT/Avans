{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71f8aa55",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "Data cleaning is a crucial step in the data analysis process. It involves identifying and correcting (or removing) errors and inconsistencies in the data to improve its quality. Clean data leads to more accurate analyses and better decision-making. In this notebook, we will perform various data cleaning tasks on our dataset to prepare it for further analysis and modeling.\n",
    "\n",
    "Our notebook will be structured in a way where we work per column of our dataset. Some columns will not be affected, we have written about this in our accountability report. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0687fe",
   "metadata": {},
   "source": [
    "## 0. Loading in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49593c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, LangDetectException\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string \n",
    "import ast\n",
    "import re \n",
    "\n",
    "file_path = Path(\"../Data/Raw/Uitgebreide_VKM_dataset.csv\")\n",
    "dataset = pd.read_csv(file_path, low_memory=False)\n",
    "dataset_before = pd.read_csv(file_path, low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e61dd61",
   "metadata": {},
   "source": [
    "## 0.1 NLP-Function\n",
    "Here we import our NLP-function we wrote in the NLP.py file. This function will be used to clean the text data in our dataset. We have two versions of the function, a hard and a light version. The hard version does stopword removal, stemming and lemmatization on the text, while the light version only does basic cleaning. We will use the hard version for our cleaning process to ensure that we have the cleanest possible data for our analysis using TF-IDF.\n",
    "\n",
    "We will use the soft version for our tuned model using sentence embeddings, as the extra steps (stopword removal, stemming and lemmatization) can remove important context from the text that is needed for the embeddings to work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4fbce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NLP import hard_nlp, soft_nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216be943",
   "metadata": {},
   "source": [
    "## 1. name\n",
    "We will use NLP (Natural Language Processing) on this column. The column is of type string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0be33668",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/danielvginneken/nltk_data'\n    - '/Volumes/External/Development/Projects/School/L2S1LU2-Recommendation/venv/nltk_data'\n    - '/Volumes/External/Development/Projects/School/L2S1LU2-Recommendation/venv/share/nltk_data'\n    - '/Volumes/External/Development/Projects/School/L2S1LU2-Recommendation/venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/External/Development/Projects/School/L2S1LU2-Recommendation/venv/lib/python3.11/site-packages/nltk/corpus/util.py:84\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     root = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/External/Development/Projects/School/L2S1LU2-Recommendation/venv/lib/python3.11/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/Users/danielvginneken/nltk_data'\n    - '/Volumes/External/Development/Projects/School/L2S1LU2-Recommendation/venv/nltk_data'\n    - '/Volumes/External/Development/Projects/School/L2S1LU2-Recommendation/venv/share/nltk_data'\n    - '/Volumes/External/Development/Projects/School/L2S1LU2-Recommendation/venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Applying the NLP function to the 'name' column\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m dataset[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mname\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhard_nlp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Check to see if both languages are processed correctly \u001b[39;00m\n\u001b[32m      5\u001b[39m dataset[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m].head(\u001b[32m15\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/External/Development/Projects/School/L2S1LU2-Recommendation/venv/lib/python3.11/site-packages/pandas/core/series.py:4943\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4808\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4809\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4810\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4815\u001b[39m     **kwargs,\n\u001b[32m   4816\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4817\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4818\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4819\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4934\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4935\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4936\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4937\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4939\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4941\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/External/Development/Projects/School/L2S1LU2-Recommendation/venv/lib/python3.11/site-packages/pandas/core/apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/External/Development/Projects/School/L2S1LU2-Recommendation/venv/lib/python3.11/site-packages/pandas/core/apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/External/Development/Projects/School/L2S1LU2-Recommendation/venv/lib/python3.11/site-packages/pandas/core/base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/External/Development/Projects/School/L2S1LU2-Recommendation/venv/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/External/Development/Projects/School/L2S1LU2-Recommendation/Notebooks/NLP.py:24\u001b[39m, in \u001b[36mhard_nlp\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     20\u001b[39m text = text.translate(\u001b[38;5;28mstr\u001b[39m.maketrans(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, string.punctuation))\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Stop words removal\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Loading stopwords\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m stopwords_en = \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwords\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     25\u001b[39m stopwords_nl = \u001b[38;5;28mset\u001b[39m(stopwords.words(\u001b[33m\"\u001b[39m\u001b[33mdutch\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     26\u001b[39m stopwords_both = stopwords_en.union(stopwords_nl)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/External/Development/Projects/School/L2S1LU2-Recommendation/venv/lib/python3.11/site-packages/nltk/corpus/util.py:120\u001b[39m, in \u001b[36mLazyCorpusLoader.__getattr__\u001b[39m\u001b[34m(self, attr)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attr == \u001b[33m\"\u001b[39m\u001b[33m__bases__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mLazyCorpusLoader object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[33m__bases__\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/External/Development/Projects/School/L2S1LU2-Recommendation/venv/lib/python3.11/site-packages/nltk/corpus/util.py:86\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     84\u001b[39m             root = nltk.data.find(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.subdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     85\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[32m     89\u001b[39m corpus = \u001b[38;5;28mself\u001b[39m.__reader_cls(root, *\u001b[38;5;28mself\u001b[39m.__args, **\u001b[38;5;28mself\u001b[39m.__kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/External/Development/Projects/School/L2S1LU2-Recommendation/venv/lib/python3.11/site-packages/nltk/corpus/util.py:81\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         root = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Volumes/External/Development/Projects/School/L2S1LU2-Recommendation/venv/lib/python3.11/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/danielvginneken/nltk_data'\n    - '/Volumes/External/Development/Projects/School/L2S1LU2-Recommendation/venv/nltk_data'\n    - '/Volumes/External/Development/Projects/School/L2S1LU2-Recommendation/venv/share/nltk_data'\n    - '/Volumes/External/Development/Projects/School/L2S1LU2-Recommendation/venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Applying the NLP function to the 'name' column\n",
    "dataset[\"name\"] = dataset[\"name\"].apply(hard_nlp)\n",
    "\n",
    "# Check to see if both languages are processed correctly \n",
    "dataset[\"name\"].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a8b3f4",
   "metadata": {},
   "source": [
    "## 2. shortdescription\n",
    "This whole column will be dropped due to it's similarity with the column \"module_tags\". We chose to drop this column instead of the module_tags column due to the fact that in places where shortdescription holds on data, module_tags does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da6afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(columns=[\"shortdescription\"], inplace=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2566bb",
   "metadata": {},
   "source": [
    "## 3. content\n",
    "This column is in all rows, except 13 of them, a copy of the description column's data. For the 13 exceptions we will add the data to the description column and then drop the content column after. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b464f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking where content and description differ\n",
    "mask_diff = dataset[\"content\"] != dataset[\"description\"]\n",
    "\n",
    "# Appending content data to description if they differ from eachother. \n",
    "def _merge_desc_content(row):\n",
    "    desc = row[\"description\"]\n",
    "    cont = row[\"content\"]\n",
    "    if pd.isna(cont):\n",
    "        return desc  # Do nothing\n",
    "    if desc == cont:\n",
    "        return desc # Dont append if the columns match\n",
    "    \n",
    "    # Appending content to description\n",
    "    return str(desc) + \" \" + str(cont)\n",
    "\n",
    "# Merging at places where content and description differ\n",
    "dataset.loc[mask_diff, \"description\"] = dataset.loc[mask_diff].apply(\n",
    "    _merge_desc_content, axis=1\n",
    ")\n",
    "\n",
    "# Showing all rows that changed, only their new description and their id value\n",
    "updated_rows = dataset.loc[mask_diff, [\"description\", \"id\"]]\n",
    "display(updated_rows.values)\n",
    "\n",
    "# Content data can be dropped after appended to description column\n",
    "dataset.drop(columns=[\"content\"], inplace=True)\n",
    "\n",
    "# Single row check\n",
    "dataset[dataset['id'] == 179].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7228b9f0",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1c88f2",
   "metadata": {},
   "source": [
    "## 4. description\n",
    "After appending the content data to the description on places where the two differ. We will now use NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75cd782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the NLP function to the 'description' column\n",
    "dataset[\"description\"] = dataset[\"description\"].apply(NLP)\n",
    "\n",
    "# Check to see if both languages are processed correctly \n",
    "print(dataset[dataset['id'] == 315][\"description\"].values)\n",
    "print(dataset[dataset['id'] == 234][\"description\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dae4267",
   "metadata": {},
   "source": [
    "## 5. location\n",
    "Location column is now of datatype string. We will change this to be an array. We do this because sometimes a string is used which says two locations. This is hard to use for filtering, better if its two string of both locations inside of an array. E.g.: \"Tilburg & Breda\" --> [\"Tilburg\", \"Breda\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea623a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get all locations inside an array and for the two 'special cases' to be two string inside of the array representing both locations\n",
    "def normalize_location(value):\n",
    "    if pd.isna(value):\n",
    "        return []\n",
    "\n",
    "    text = str(value).strip()\n",
    "\n",
    "    # Special cases\n",
    "    if text == \"Breda en Den Bosch\":\n",
    "        return [\"Breda\", \"Den Bosch\"]\n",
    "    if text == \"Den Bosch en Tilburg\":\n",
    "        return [\"Den Bosch\", \"Tilburg\"]\n",
    "\n",
    "    # Default will just convert the single location to be inside of an array for data type consistency\n",
    "    return [text]\n",
    "\n",
    "dataset[\"location\"] = dataset[\"location\"].apply(normalize_location)\n",
    "\n",
    "# Check where both scenarios can be seen\n",
    "dataset.head(183)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288297f8",
   "metadata": {},
   "source": [
    "## 6. learningoutcomes\n",
    "Here we will also use NLP to prepare the data for our model later. However, we also found some inconsistenies inside of the data of this column. You can find values such as: 'ntb', 'nog te bepalen, nan, etc. These values will all have to be properly set to NaN values so that our model later does not take 'nog te bepalen' as an input.\n",
    "\n",
    "First we will remove the inconsistensies, after we will use NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f660e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we need to remove/set to np.nan inside of the learningoutcomes column\n",
    "error_values_learningoutcomes_contains = [\"ntb\", \"nog niet bekend\", \"nog te formuleren\", \"nog nader te bepalen\", \"nader te bepalen\", \"nog te bepalen\", \"n.n.b.\"]\n",
    "error_values_learningoutcomes_specific = [\"volgt\", \"nan\"]\n",
    "\n",
    "# Setting all text to lower casing\n",
    "dataset[\"learningoutcomes\"] = dataset[\"learningoutcomes\"].str.lower()\n",
    "\n",
    "# Looping and setting values to np.nan for contain errors and specific errors\n",
    "for val in error_values_learningoutcomes_contains:\n",
    "    # Only match the full phrase anywhere in the text\n",
    "    dataset.loc[dataset[\"learningoutcomes\"].str.contains(re.escape(val), na=False), \"learningoutcomes\"] = np.nan\n",
    "\n",
    "# Now handle the specific list with exact matches\n",
    "dataset.loc[dataset[\"learningoutcomes\"].isin(error_values_learningoutcomes_specific), \"learningoutcomes\"] = np.nan\n",
    "\n",
    "# Checking is succesful --> number 6, 7, 8 have all been converted to NaN succesfully. Before they were all value: \"Nader te bepalen\". ALso checked some random locations. \n",
    "dataset.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fa4a43",
   "metadata": {},
   "source": [
    "Now for the NLP part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450b2be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the NLP function to the 'learningoutcomes' column\n",
    "dataset[\"learningoutcomes\"] = dataset[\"learningoutcomes\"].apply(NLP)\n",
    "\n",
    "# Check to see if both languages are processed correctly \n",
    "print(dataset[dataset['id'] == 315][\"learningoutcomes\"].values)\n",
    "print(dataset[dataset['id'] == 234][\"learningoutcomes\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e88a3e",
   "metadata": {},
   "source": [
    "## 7. color-coded columns\n",
    "We drop the colour columns (Green, Blue, Red, Yellow) as they are not relevant for our recommendation system mainly because it is not useful metadata for understanding user preferences or item characteristics and most values in these columns are Not A Number (NaN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a696541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all 4 columns\n",
    "dataset.drop(columns=[\"Rood\"], inplace=True)\n",
    "dataset.drop(columns=[\"Groen\"], inplace=True)\n",
    "dataset.drop(columns=[\"Blauw\"], inplace=True)\n",
    "dataset.drop(columns=[\"Geel\"], inplace=True)\n",
    "\n",
    "# Check\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9885542e",
   "metadata": {},
   "source": [
    "## 8. module_tags\n",
    "NLP will once again be used here, but first we need to set all of the empty rows ( [], ['ntb'] ) to np.nan. We do this because these rows provide no information and will mess up the model if we don't correct count them out with np.nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567adeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string representation of lists to actual lists\n",
    "dataset[\"module_tags\"] = dataset[\"module_tags\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Now set NaN for empty lists or ['ntb']\n",
    "dataset.loc[dataset[\"module_tags\"].apply(lambda x: x == [] or x == ['ntb']), \"module_tags\"] = np.nan\n",
    "\n",
    "# this position was ['ntb'] before, now we can check to see if it's fixed. Ofcourse we also check on several other locations and other cases like []\n",
    "dataset.iloc[20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccd9b35",
   "metadata": {},
   "source": [
    "After properly handling the empty data rows. We now have to get the single string outside of their array and append them into 1 big string so that we can use our NLP function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf22cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"module_tags\"] = dataset[\"module_tags\"].apply(\n",
    "    lambda x: \" \".join(x) if isinstance(x, list) else x\n",
    ")\n",
    "dataset[\"module_tags\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03119c2",
   "metadata": {},
   "source": [
    "Now for the NLP part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7edaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the NLP function to the 'module_tags' column\n",
    "dataset[\"module_tags\"] = dataset[\"module_tags\"].apply(NLP)\n",
    "\n",
    "# Check to see if both languages are processed correctly \n",
    "print(dataset[dataset['id'] == 315][\"module_tags\"].values)\n",
    "print(dataset[dataset['id'] == 234][\"module_tags\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b9945f",
   "metadata": {},
   "source": [
    "## 9. popularity_score\n",
    "The scores of this column range from 0-500. We will normalize them on a scale from 0-1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6178c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide scores by max score\n",
    "dataset[\"popularity_score\"] = dataset[\"popularity_score\"] / 500\n",
    "\n",
    "# Check\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307ce8b6",
   "metadata": {},
   "source": [
    "## 10. Finalizing\n",
    "\n",
    "We have completed the data cleaning process for our dataset. The cleaned data is now ready for modeling. We will save the cleaned dataset as a new CSV file to ensure that we can easily access and use it in future steps of our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8fa30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(Path(\"../Data/Cleaned/cleaned_dataset.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a1228e",
   "metadata": {},
   "source": [
    "This is the final structure and data of our cleaned dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fdcd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64b46fa",
   "metadata": {},
   "source": [
    "This was what it looked like before all the changes we made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d4cb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_before.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf489465",
   "metadata": {},
   "source": [
    "## !!Important mention\n",
    "We also exported a cleaned dataset where our NLP function was altered to not remove any stopwords, nor perform lemmetization and stemming. This is because sentence embedding works best when you keep closer to original text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
